{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "colab": {
      "name": "DL_Lab9.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIoAO7fGWiro",
        "colab_type": "text"
      },
      "source": [
        " <a href=\"https://colab.research.google.com/github/TimWhiting/DeepLearning/blob/master/DL_Lab9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dUAQaztWirv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from itertools import chain\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms, utils, datasets\n",
        "from tqdm import tqdm\n",
        "from torch.nn.parameter import Parameter\n",
        "import pdb\n",
        "import torchvision\n",
        "from functools import reduce\n",
        "import os\n",
        "import gzip\n",
        "import tarfile\n",
        "import gc\n",
        "from PIL import Image\n",
        "import io\n",
        "from IPython.core.ultratb import AutoFormattedTB\n",
        "__ITB__ = AutoFormattedTB(mode='Verbose', color_scheme='LightBg', tb_offset=1)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cpQoGywWir3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EmbeddingNetwork(nn.Module):\n",
        "  def __init__(self, state_dim, hidden_dim, output_dim):\n",
        "    super(EmbeddingNetwork, self).__init__()\n",
        "    self.net = nn.Sequential(\n",
        "      nn.Linear(input_dim, output_dim/2),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(output_dim/2, output_dim),\n",
        "      nn.ReLU(),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5-LddEmWir8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ValueNetwork(nn.Module):\n",
        "  def __init__(self, state_dim, hidden_dim):\n",
        "    super(ValueNetwork, self).__init__()\n",
        "    self.net = nn.Sequential(\n",
        "      nn.Linear(state_dim, hidden_dim),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(hidden_dim, hidden_dim),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(hidden_dim, 1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rret6kpcWisA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PolicyNetwork(nn.Module):\n",
        "  def __init__(self, state_dim, hidden_dim, action_dim):\n",
        "    super(PolicyNetwork, self).__init__()\n",
        "    self.net = nn.Sequential(\n",
        "      nn.Linear(state_dim, hidden_dim),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(hidden_dim, hidden_dim),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(hidden_dim, action_dim),\n",
        "      nn.Softmax(dim=1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Bv2G1tOWisE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class AdvantageDataset(Dataset):\n",
        "  def __init__(self, experience):\n",
        "    super(AdvantageDataset, self).__init__()\n",
        "    self._exp = experience\n",
        "    self._num_runs = len(experience)\n",
        "    self._length = reduce(lambda acc, x: acc + len(x[0]), experience, 0)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    idx = 0\n",
        "    seen_data = 0\n",
        "    current_exp = self._exp[0][0]\n",
        "    while seen_data + len(current_exp) - 1 < index:\n",
        "      seen_data += len(current_exp)\n",
        "      idx += 1\n",
        "      current_exp = self._exp[idx][0]\n",
        "    chosen_exp = current_exp[index - seen_data]\n",
        "    # What should be returned is the action taken and the advantage for that?\n",
        "    return chosen_exp\n",
        "\n",
        "  def __len__(self):\n",
        "    return self._length\n",
        "\n",
        "  def averageLength(self):\n",
        "    return self._length/self._num_runs\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYP_lR7vWisI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9b3e9c08-4bec-4af0-8b4e-4aabc902d338"
      },
      "source": [
        "def select_action(probabilities):\n",
        "  batch_size = probabilities.shape[0]\n",
        "  actions = np.empty((batch_size, 1), dtype=np.uint8)\n",
        "  probs_np = probabilities.cpu().detach().numpy()\n",
        "  for i in range(batch_size):\n",
        "      action_one_hot = np.random.multinomial(1, probs_np[i])\n",
        "      action_idx = np.argmax(action_one_hot)\n",
        "      actions[i, 0] = action_idx\n",
        "  return actions\n",
        "\n",
        "def likelihood_of_getting(indicies, distribution):\n",
        "  try:\n",
        "    return distribution[range(distribution.shape[0]), indicies.long()[:, 0]].unsqueeze(1)\n",
        "  except Exception as e:\n",
        "    print(\"Indicies {}\".format(indicies.size()))\n",
        "    print(\"Distribution {}\".format(distribution.size()))\n",
        "    print(\"Here\")\n",
        "    raise Exception(e)\n",
        "    pass\n",
        "    # print(indicies.size())\n",
        "    # print(distribution.size())\n",
        "\n",
        "def mean(values):\n",
        "  return sum(values)/len(values)\n",
        "\n",
        "def discrete_entropy(array):\n",
        "    log_prob = torch.log(array)\n",
        "    return -torch.sum(log_prob * array, dim=1, keepdim=True)\n",
        "\n",
        "def main(device):\n",
        "  env = gym.make('CartPole-v0')\n",
        "  policy = PolicyNetwork(4, 256, 2).to(device)\n",
        "  value = ValueNetwork(4, 256).to(device)\n",
        "\n",
        "  policy_optim = optim.Adam(chain(policy.parameters(), value.parameters()), lr=3e-4, weight_decay=0.01, betas=(0.9, 0.999))\n",
        "\n",
        "  # ... more stuff here...\n",
        "  value_criterion = nn.MSELoss()\n",
        "\n",
        "  # Hyperparameters\n",
        "  epochs = 1000  # 1000\n",
        "  env_samples = 100\n",
        "  steps_to_take = 200\n",
        "  gamma = 0.99\n",
        "  value_epochs = 4\n",
        "  batch_size = 32\n",
        "  epsilon = 0.2\n",
        "  entropy_coef = 1e-3\n",
        "  ppo_pos = 1 + epsilon\n",
        "  ppo_neg = 1 - epsilon\n",
        "\n",
        "  loop = tqdm(total=epochs, position=0, leave=False)\n",
        "\n",
        "  for _ in range(epochs):\n",
        "    rewards = []\n",
        "    value_losses = []\n",
        "    policy_losses = []\n",
        "    entropy_losses = []\n",
        "    # generate rollouts\n",
        "    rollouts = []\n",
        "    for sample_num in range(env_samples):\n",
        "      \n",
        "      state = env.reset()\n",
        "      rollout_reward = 0\n",
        "      rollout_experience = []\n",
        "      for step in range(steps_to_take):\n",
        "        # if sample_num == 0:\n",
        "        #   env.render()\n",
        "        # don't forget to reset the environment at the beginning of each episode!\n",
        "        # rollout for a certain number of steps!\n",
        "        state_in = torch.as_tensor(state).float().unsqueeze(0)\n",
        "        # print(state_in.size())\n",
        "        action_distribution = policy(state_in)\n",
        "        # your agent here (this takes random actions)\n",
        "        action = select_action(action_distribution)[0]\n",
        "        # print(action_distribution.size())\n",
        "        # print(action[0])\n",
        "        # print(env.action_space)\n",
        "        # print(env)\n",
        "        new_state, reward, done, info = env.step(int(action))\n",
        "        if done:\n",
        "          reward = 0\n",
        "          discounted_reward = 0\n",
        "          rollout_experience.append(\n",
        "              (state, reward, done, action, action_distribution.detach().numpy()[0], discounted_reward))\n",
        "          state = new_state\n",
        "          rollout_reward += 0\n",
        "          break\n",
        "        discounted_reward = (gamma**(steps_to_take-step))*reward\n",
        "        \n",
        "        rollout_experience.append(\n",
        "            (state, reward, done, action, action_distribution.detach().numpy()[0], discounted_reward))\n",
        "        state = new_state\n",
        "        rollout_reward += (gamma**(steps_to_take-step))*reward\n",
        "\n",
        "      rollouts.append((rollout_experience, rollout_reward))\n",
        "\n",
        "    # print('avg standing time:', standing_len / env_samples)\n",
        "\n",
        "    # Approximate the value function\n",
        "    value_dataset = AdvantageDataset(rollouts)\n",
        "    value_loader = DataLoader(\n",
        "        value_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "    for _ in range(value_epochs):\n",
        "      for value_data in value_loader:\n",
        "        policy_optim.zero_grad()\n",
        "        state, reward, done, action, action_distribution, discounted_reward = value_data\n",
        "        state_tensor = torch.as_tensor(state).detach().float()\n",
        "        reward_float = torch.as_tensor(discounted_reward).unsqueeze(1).float()\n",
        "        expected_ret = value(state_tensor)\n",
        "\n",
        "        value_loss = value_criterion(expected_ret, reward_float)\n",
        "        advantage = reward_float - expected_ret.detach()\n",
        "        current_action_distribution = policy(state_tensor)\n",
        "        likelihood_new = likelihood_of_getting(action, current_action_distribution)\n",
        "        likelihood_old = likelihood_of_getting(action, action_distribution)\n",
        "        if likelihood_new is None or likelihood_old is None:\n",
        "          print(\"Problem!\")\n",
        "          continue\n",
        "        ratio = likelihood_new/likelihood_old\n",
        "        policy_calc = torch.min(advantage*ratio, torch.clamp(ratio, ppo_neg, ppo_pos)*advantage)\n",
        "        policy_loss = -torch.mean(policy_calc)\n",
        "        entropy_loss = -entropy_coef * torch.mean(discrete_entropy(current_action_distribution))\n",
        "        rewards.append(torch.mean(reward_float).item())\n",
        "        policy_losses.append(policy_loss.item())\n",
        "        value_losses.append(value_loss.item())\n",
        "        entropy_losses.append(entropy_loss.item())\n",
        "        total_loss = policy_loss + .1*value_loss + entropy_loss\n",
        "        \n",
        "        total_loss.backward()\n",
        "        policy_optim.step()\n",
        "\n",
        "    loop.set_description('Reward: {}, Value: {}, Policy: {}, Entropy: {}, Average Length: {}'.format(mean(rewards), mean(value_losses), mean(policy_losses), mean(entropy_losses), value_dataset.averageLength()))\n",
        "    loop.update()\n",
        "  env.close()\n",
        "main('cpu')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reward: 0.14853906407952308, Value: 0.0015484575650979032, Policy: -0.007003044583793131, Entropy: -0.0006931119596369431, Average Length: 22.19:  42%|████▏     | 420/1000 [17:50<16:06,  1.67s/it]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zr96mDObWisL",
        "colab_type": "text"
      },
      "source": [
        ".3257\n",
        "\n"
      ]
    }
  ]
}