{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "# Deep Learning Part 3: Datasets, Data Loading, Cross Entropy, and Convolutional Networks\n",
    "\n",
    "## Grading Standards:\n",
    "*  10%: Dataset/MNIST section\n",
    "*  10%: Correct implementation and use of cross entropy loss\n",
    "*  20%: Correct training/validation functions\n",
    "*  40%: Successful training and validation with MLP and convolution networks\n",
    "*  12%: Convolutional layer quiz\n",
    "*   8%: Comparison between MLP and convolution networks\n",
    "___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wQOefmcZVgTl"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUHlLRJ0Q9Fz"
   },
   "source": [
    "Set your global variable `device`, using the `torch.device()` function. In order to use cuda remember to request a GPU from Runtime > Change Runtime.\n",
    "\n",
    "***Important Note**: If you spend too much time or memory on the GPU in Google Colab then you will be timed out. This may not be a big deal with this lab, but it can become a big deal in later labs. It is recommended to set your `device` and Runtime to CPU first and once everything in the lab is working properly to set it to the GPU.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kFAeNl0mQ9F0"
   },
   "outputs": [],
   "source": [
    "device = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8plxzBbQ9F1"
   },
   "source": [
    "---\n",
    "\n",
    "# Datasets and Data Loading\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6A1ZwnczQ9F1"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sP7rpMjiQ9F2"
   },
   "source": [
    "We are going to make a PyTorch `Dataset`.\n",
    "There are three parts to creating a `Dataset`:\n",
    "1. `__init__()`: This is where you get all relevant data for your dataset.\n",
    "2. `__len__()`: You return how large your dataset is.\n",
    "3. `__getitem__()`: You return an item from your dataset given an index.\n",
    "\n",
    "Implement the TODOs below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x3SGWrkbQ9F3"
   },
   "outputs": [],
   "source": [
    "class SineDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # TODO: The code is same from lab 2, so you can uncomment the code below.\n",
    "        # self.x = torch.rand((100,1))*8 - 4\n",
    "        # self.y = torch.sin(self.x) + torch.randn_like(self.x)*.1 # the second part of the sum adds noise to the function\n",
    "\n",
    "    def __len__(self):\n",
    "        # TODO: Return the len of your dataset.\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # TODO: i will be an index so return x_i and y_i\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQtRsLxWQ9F3"
   },
   "source": [
    "We will now create a `SineDataset` and print out the length of dataset, i.e. `len(dataset)`, and the item in your dataset at index 0, i.e. `dataset[0]`.\n",
    "\n",
    "*Note: `__len__()` and `__getitem__()` are private methods and should not be called directly.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M1HD_XafQ9F4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Ow7q2UEQ9F4"
   },
   "source": [
    "A `DataLoader` uses the `__len__` and `__getitem__` of a `Dataset` to sample indices in `[0, ..., len(dataset)-1]` and collect a batch of items from the `Dataset`.\n",
    "The `DataLoader` will then try to convert the sampled entries into tensors (if they are not already) and concatenate them together.\n",
    "Create a `DataLoader` object below; pass in your dataset and `batch_size=32` as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8G9BkZUgQ9F4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHE3O4wwQ9F5"
   },
   "source": [
    "Iterate through your dataloader with a for loop. Because `SineDataset.__getitem__()` returns two items, the for loop will return a tuple.\n",
    "Either unpack the entries in your tuple in the for loop:\n",
    "```python\n",
    "for x, y in dataloader\n",
    "```\n",
    "or after the loop:\n",
    "```python\n",
    "for batch in dataloader:\n",
    "    x, y = batch\n",
    "```\n",
    "\n",
    "Print out the shapes of `x` and `y` for each batch in the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zxJ-K52QQ9F5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9XGi82cQ9F5"
   },
   "source": [
    "You will notice that the shapes are `(B, Z_in)`, where `B` is batch size and `Z_in` is our input feature size, which is exactly what we want.\n",
    "Also note that the last batch has a batch size of 4; this is because the `DataLoader` samples **without replacement** and these are the last items in our `Dataset` that have not been sampled.\n",
    "\n",
    "Let's now create our real dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORW8H-QwQ9F6"
   },
   "source": [
    "---\n",
    "\n",
    "# MNIST\n",
    "\n",
    "We are now going to look at the MNIST dataset, which is a dataset of handwritten numbers.\n",
    "Our objective will be to create a neural network that can predict the number given the image.\n",
    "\n",
    "First import `torchvision` below so we can retrieve the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ybgv1p4PQ9F6"
   },
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QroZnsvvQ9F7"
   },
   "source": [
    "You can use `torchvision.datasets.MNIST()` to download the MNIST dataset (which inherits the `Dataset` class).\n",
    "For arguments, specify `root=\"/tmp/\"` to denote the location, `train=True` or `train=False` to get the training or test dataset, `download=True` to specify you want to download the dataset, and `transform=torchvision.transforms.ToTensor()` to convert the MNIST images from PIL images to PyTorch tensors.\n",
    "Create both a `train_dataset` and `val_dataset`.\n",
    "\n",
    "*Note: It is good practice to use a train, val, and test dataset, especially in the real world, but in this class we will mainly focus on train and val datasets to simplify things.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BeJcijXkQ9F7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujysG9TCQ9F7"
   },
   "source": [
    "Print out the lengths of `train_dataset` and `val_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61Px30_LQ9F8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fViNzhuQ9F8"
   },
   "source": [
    "Grab element 0 from the `train_dataset`. As a heads up, like our `SineDataset`, `MNIST` returns an image `x` and a class/target `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3QUod3d5Q9F8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEJ45xaVQ9F8"
   },
   "source": [
    "Use the `type()` function to see what type of object `x` and `y` are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BH7jijNxQ9F8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8xFEKjZQ9F9"
   },
   "source": [
    "Since x is a tensor, print out its `.dtype`, `.shape`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nspQc1uSQ9F9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XV-K1HjMQ9F9"
   },
   "source": [
    "The shape of a tensor image is `(C, H, W)`, where `C` is channels, `H` is height, and `W` is width.\n",
    "In our case `x` has 1 channel and it is a 28x28 image.\n",
    "Because there is 1 channel, it is likely that the image is grayscale (which it is).\n",
    "\n",
    "Now visualize the image and display its class.\n",
    "Use the `plt.imshow()` function to visualize `x`; add the argument `cmap=\"gray\"` to denote the image is grayscale.\n",
    "Use the `plt.title()` function to set the title of the `plt` image to the class `y`.\n",
    "\n",
    "*Note: `plt` expects grayscale images to only have to dimension, HxW, so `.squeeze()` the 0th dimension*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xq6qHfNZQ9F9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQ7Qc7N5Q9F9"
   },
   "source": [
    "Print out the min and max values of `x` using `torch.min()` and `torch.max()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0fU7fzKGQ9F-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLK7CH_YQ9F-"
   },
   "source": [
    "Our tensors are normalized between 0 and 1, which is good so we don't have to do any normalization.\n",
    "Now that we have a better understanding of our image data, let's examine the classes in the dataset.\n",
    "\n",
    "Create a `get_dataset_classes()` function which takes a dataset as input and count how many times each class appears.\n",
    "Return a dictionary where the keys are the classes and the values represent the number of times each class appears in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLGxbOj3Q9F-"
   },
   "outputs": [],
   "source": [
    "def get_dataset_classes(dataset):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNlLGb2pQ9F-"
   },
   "source": [
    "Execute the code below to visualize the dataset classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ZoP7o0JQ9F_"
   },
   "outputs": [],
   "source": [
    "train_classes = get_dataset_classes(train_dataset)\n",
    "val_classes = get_dataset_classes(val_dataset)\n",
    "\n",
    "def plot_classes(ax, classes, title):\n",
    "    ax.bar(classes.keys(), classes.values())\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(list(range(10)))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "plot_classes(axes[0], train_classes, \"Train Dataset Classes\")\n",
    "plot_classes(axes[1], val_classes, \"Val Dataset Classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lfg-UTydQ9F_"
   },
   "source": [
    "There are 10 classes, which makes sense because there are 10 digits, 0-9.\n",
    "Furthermore, the classes are pretty evenly spread on both datasets, which means our baseline accuracy is roughly 10%.\n",
    "\n",
    "Now create two `DataLoader` objects called, `train_loader` and `val_loader`.\n",
    "The `train_loader` should have your `train_dataset` and the `val_loader` should have your `val_dataset`.\n",
    "Set the `batch_size` of both dataloaders equal to 32 and set `shuffle=True` for the `train_loader` so that the dataset is shuffled every time.\n",
    "To improve the speed at which you `DataLoader`s can load the data, set `num_workers=4` (for multiprocessing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mLp4yWwXQ9F_"
   },
   "outputs": [],
   "source": [
    "train_loader = None\n",
    "val_loader = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cEGFonGnQ9GH"
   },
   "source": [
    "Now use `x, y = next(iter(train_loader))` to get a single batch of data from the `train_loader` and print out the shapes and dtypes of `x` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_XsyFmUQ9GH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpSxXIkQQ9GI"
   },
   "source": [
    "The shape of `x` is `(B, C, H, W)`, where `B` is the batch size. **Always remember, in PyTorch, your data should have a batch dimension.**\n",
    "The shape of `y` is `(B,)` and it is a tensor of type `long`, which is what we want because we are doing classification, which means we want our target to be a class label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QY4owfQwm-Ni"
   },
   "source": [
    "___\n",
    "\n",
    "### MLP Network\n",
    "\n",
    "We are now going to make our network.\n",
    "Because we are doing image classification, the input to our network is a batch of images, `shape=(B, C, H, W)`, and the output of our network is a batch of probabilities, `shape=(B, K)`, where `K` represents the number classes in our dataset.\n",
    "In our case `K=10`.\n",
    "\n",
    "We will first try to solve this problem using a fully connected deep network (like your DeepNet from lab 2), sometimes called a Multi-Layer Perceptron (MLP).  \n",
    "Implement an `MLP` below (don't forget to use `nn.Sequential`, `nn.Linear`, `nn.ReLU`).\n",
    "Because `nn.Linear` expects tensors of shape `(B, Z)`, where `Z` is the input feature size, we need to flatten our images. Use the `.view()` function to reshape `x.shape=(B, C, H, W)` into `x.shape=(B, Z)`, where `Z=C*H*W`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ljx0drnxQ9GI"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, out_features, hidden_size):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJIluYrhQ9GI"
   },
   "source": [
    "Because we are doing classification, we need to use a different loss function that MSE; cross entropy loss is a good choice and is a common loss function for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trSdeZy2Q9GJ"
   },
   "source": [
    "---\n",
    "\n",
    "# Softmax and CrossEntropy\n",
    "\n",
    "The cross entropy (CE) function is $CE(p, q) = - \\sum p(x) \\log q(x)$, where $p$ and $q$ are probability functions ($p$ is the target probabilities and $q$ is the predicted probabilities) and in our case $x$ represents a class.\n",
    "\n",
    "$p$ represents the target distribution, the true class distribution, which means it is a one-hot vector $p_c$, where $c$ represents the index of the class:\n",
    "$$p_i = \\begin{cases}\n",
    "1, & \\textrm{if } i = c \\\\\n",
    "0, & \\textrm{if } i \\not = c \\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "Then $CE(p, q) = - \\sum_i p_i \\log q_i$ will become $CE(p, q) = - p_c \\log q_c$, because $p_{i \\not = c} = 0$, which is further reduced to $CE(p, q) = - \\log q_c$, since $p_c = 1$. In other words, cross entropy loss for classification is the negative log of the predicted probability of the correct class.\n",
    "\n",
    "Therefore, $p$ is never passed into `F.cross_entropy_loss()`, instead you pass in $q$ you predicted distribution and $c$ the index of the correct class.\n",
    "\n",
    "Implement `prenormalized_cross_entropy_loss` below. $q$ is assumed to be a normalized probability distribution.\n",
    "\n",
    "*Note 1: Do **not** use a for loop. You can index into a tensor with array slicing (hint: You will need to use `torch.arange()` for the 0th dimension of `q`)*\n",
    "\n",
    "*Note 2: Compute the mean cross entropy of the batch not the sum* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xuhEjtx7Q9GJ"
   },
   "outputs": [],
   "source": [
    "def prenormalized_cross_entropy_loss(q, c):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqzoiQDgQ9GK"
   },
   "source": [
    "Validate your function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5l96Vh8HQ9GK"
   },
   "outputs": [],
   "source": [
    "def test_prenormalized_cross_entropy_loss():\n",
    "    q = torch.tensor([[.1, .5, .4],\n",
    "                      [.2, .2, .6],\n",
    "                      [.3, .3, .3]])\n",
    "    c = torch.tensor([2, 0, 1])\n",
    "    assert torch.allclose(prenormalized_cross_entropy_loss(q, c), torch.tensor(1.2432), atol=1e-4)\n",
    "\n",
    "test_prenormalized_cross_entropy_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4pxw0AhQ9GL"
   },
   "source": [
    "This assumed that `q` was a probability distribution, but usually neural networks output logits $l$, which are unnormalized probabilities.\n",
    "One way we could normalize our logits $l$ into probabilities $q$ is to divide $l$ by the sum of logits $q_i = \\frac{l_i}{\\sum_j l_j}$, but that doesn't work if $l_i$ is negative.\n",
    "The softmax, which exponentiates the $logit$, $q_i = \\frac{e^{l_i}}{\\sum_j e^{l_j}}$, before dividing by the sum of exponentiated logits removes the issues of negativity (there are other good reasons for using softmax, such as numerical stability).\n",
    "\n",
    "However, applying `q = softmax(l)` to `prenormalized_cross_entropy_loss(q, c)` can still be numerically unstable.\n",
    "Luckily, we can simplify our function:\n",
    "$$\\begin{align}\n",
    "CE(l, c) &= - \\log \\frac{e^{l_c}}{\\sum_j e^{l_j}} \\\\\n",
    "&= - (\\log e^{l_c} - \\log \\sum_j e^{l_j}) \\\\\n",
    "&= - (l_c - \\log \\sum_j e^{l_j}) \\\\\n",
    "&= - l_c + \\log \\sum_j e^{l_j}\n",
    "\\end{align}$$\n",
    "\n",
    "While you could implement $\\log \\sum_j e^{l_j}$, you should use `torch.logsumexp()` which will exponentiate, sum, and then log your logits, but in a more numerically stable way.\n",
    "Implement `cross_entropy_loss()` below.\n",
    "You can validate it works by comparing it with the output of `F.cross_entropy()`.\n",
    "\n",
    "*Note: Do **not** use a for loop*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eqBTmzaWQ9GL"
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loss(l, c):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBEOn1gKQ9GL"
   },
   "source": [
    "It is hard to tell how well a model is performing just from its cross entropy loss, so create a `get_accuracy()` function to measure accuracy.\n",
    "`get_accuracy()` takes in a `y_hat` and `y`, where `y_hat` contains the predicted logits (unnormalized probabilities) for some images `x` and `y` are the labels.\n",
    "You can get the predicted label from `y_hat`, by using the `torch.argmax()` function.\n",
    "\n",
    "*Note: Do **not** use a for loop.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aDoJ9UewQ9GL"
   },
   "outputs": [],
   "source": [
    "def get_accuracy(y_hat, y):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gliu8YgjQ9GL"
   },
   "source": [
    "## Validation and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how well training is going, implement a `validation()` function to compute the average loss and accuracy over all instances in the `val_loader`. \n",
    "This function will look very similar to a basic training loop, but without any optimization, e.g. no `loss.backward()` or `optimizer.step()`. \n",
    "To speed up the process use `torch.no_grad()` to keep PyTorch from building the computation graph.\n",
    "You can use `torch.no_grad()` either as a decorator:\n",
    "```python\n",
    "@torch.no_grad() \n",
    "def fn():\n",
    "    ...\n",
    "```\n",
    "or as a context manager:\n",
    "```python\n",
    "def fn(): \n",
    "    with torch.no_grad():\n",
    "        ...\n",
    "```\n",
    "\n",
    "*Remember you can use `.item()` on a tensor with one element to convert it into a float/int.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eVpKK9_bQ9GM"
   },
   "outputs": [],
   "source": [
    "def validation(net, val_loader):\n",
    "    # TODO: Return the network's average loss and accuracy on the val loader\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RHZYEKNQ9GM"
   },
   "source": [
    "Now implement the `train()` function.\n",
    "This will look similar to the `train()` you implemented in lab 2, but now you will also store training accuracies, and at every `log_val_interval` you will call `validation` and store the validation loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7PQFG1zAQ9GM"
   },
   "outputs": [],
   "source": [
    "def train(net, train_loader, val_loader, optimizer, n_optimization_steps, log_val_interval):\n",
    "    # TODO: Implement training loop and return the training and validation losses and accuracies.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpKn33eFQ9GM"
   },
   "source": [
    "Now train an `MLP` on `MNIST`. To speed up training use the `torch.optim.Adam` optimizer instead of `torch.optim.SGD`.\n",
    "\n",
    "*Note: Don't forget to put your network on `device`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PaunNy19Q9GN"
   },
   "outputs": [],
   "source": [
    "# TODO: Set torch seed for reproducibility\n",
    "\n",
    "# TODO: Initialize your MLP, called net\n",
    "net = None\n",
    "\n",
    "# TODO: Create an Adam optimizer (lr=.001 works well)\n",
    "\n",
    "# TODO: Train your MLP for 2000 steps and set log_val_interval=50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qxlBHEY5Q9GN"
   },
   "source": [
    "Now plot your training and validation loss on the same plot and plot your training and validation accuracies on the same plot.\n",
    "Properly set your x- and y-axis labels and create a legend to make your plot legible.  \n",
    "\n",
    "*Note: that you can specify the x-values for each point by calling `plt.plot(x, y)` instead of `plt.plot(y)`. Since you store validation every 50 steps, you'll need to use `torch.arange` to get the proper x-values to align the validation results to the training results.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "janqsjw6Q9GN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ronkEckHiDaU"
   },
   "source": [
    "___\n",
    "\n",
    "# Convolution Networks\n",
    "\n",
    "Now we will create a convolution network.\n",
    "When we were dealing with an `MLP` we used fully-connected `nn.Linear` layers.\n",
    "In a convolution network we use `nn.Conv2d` layers.\n",
    "`nn.Linear` maps tensors of shape `(B, F_in) -> (B, F_out)`.\n",
    "`nn.Conv2d` maps tensors of shape `(B, C_in, H_in, W_in) -> (B, C_out, H_out, W_out)`, where `C_in` represents our input channels and `C_out` represents our output channels.\n",
    "You decide what `C_out` should be when you initialize.\n",
    "The mapping that\n",
    "\n",
    "If we had a batch of `img` tensors with shape `(B, 3, 8, 8)` and we wanted it to become `(B, 6, 4, 4)` we could create a convolution layer:\n",
    "```python\n",
    "conv_layer = nn.Conv2d(in_channels=3,  # Our 'C_in' which is 3\n",
    "                       out_channels=6, # Our desired 'C_out'\n",
    "                       kernel_size=2,  # One way to make an 8x8 image become a 2x2 image is to have the kernel be 2x2,\n",
    "                       padding=0,      #    with zero padding,\n",
    "                       stride=2,       #    and a stride of 2.\n",
    "                      )\n",
    "```\n",
    "Validate this is true below by creating `conv_prac()` function, which creates a random tensor with shape `(B, 3, 8, 8)`, passes it through `conv_layer`, and prints out the resulting shape.\n",
    "\n",
    "*Note: `kernel_size`, `padding`, and `stride` can all be tuples in case you want different (height, width) parameters, e.g. `kernel=(kernel_height, kernel_width)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0WFcZkkWQ9GN"
   },
   "outputs": [],
   "source": [
    "def conv_prac():\n",
    "    pass\n",
    "\n",
    "conv_prac()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7hXGRxUQh9gX"
   },
   "source": [
    "___\n",
    "\n",
    "### Quiz\n",
    "Test your knowledge of how convolution layers affect the shape of outputs by answering the following quiz questions.\n",
    "\n",
    "\n",
    "*Using a Kernel size of 3×3 what should the settings of your 2d convolution be that results in the following mappings (first answer given to you)*\n",
    "\n",
    "* (c=3, h=10, w=10) ⇒ (c=10, h=8, w=8) : **(out_channels=10, kernel_size=(3, 3), padding=(0, 0))**\n",
    "* (c=3, h=10, w=10) ⇒ (c=22, h=10, w=10) : **Your answer in bold here**\n",
    "* (c=3, h=10, w=10) ⇒ (c=65, h=12, w=12) : **Your answer in bold here**\n",
    "* (c=3, h=10, w=10) ⇒ (c=7, h=20, w=20) : **Your answer in bold here**\n",
    "\n",
    "*Using a Kernel size of 5×5:*\n",
    "\n",
    "* (c=3, h=10, w=10) ⇒ (c=10, h=8, w=8) : (out_channels=10, kernel_size=(5, 5), padding=(1, 1))\n",
    "* (c=3, h=10, w=10) ⇒ (c=100, h=10, w=10) : **Your answer in bold here**\n",
    "* (c=3, h=10, w=10) ⇒ (c=23, h=12, w=12) : **Your answer in bold here**\n",
    "* (c=3, h=10, w=10) ⇒ (c=5, h=24, w=24) : **Your answer in bold here**\n",
    "\n",
    "*Using Kernel size of 5×3:*\n",
    "\n",
    "* (c=3, h=10, w=10) ⇒ (c=10, h=8, w=8) : **Your answer in bold here**\n",
    "* (c=3, h=10, w=10) ⇒ (c=100, h=10, w=10) : **Your answer in bold here**\n",
    "* (c=3, h=10, w=10) ⇒ (c=23, h=12, w=12) : **Your answer in bold here**\n",
    "* (c=3, h=10, w=10) ⇒ (c=5, h=24, w=24) : **Your answer in bold here**\n",
    "\n",
    "*Determine the kernel that requires the smallest padding size to make the following mappings possible:*\n",
    "\n",
    "* (c=3, h=10, w=10) ⇒ (c=10, h=9, w=7) : **Your answer in bold here**\n",
    "* (c=3, h=10, w=10) ⇒ (c=22, h=10, w=10) : **Your answer in bold here**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X6iWe-vrQ9GO"
   },
   "source": [
    "## ConvNet\n",
    "\n",
    "Now create a convolution network `ConvNet` that inherits `nn.Module`.\n",
    "The network should have 3 convolution layers (each layer should have 16 output channels):\n",
    "1. Conv Layer 1 should have a 6x6 kernel, no padding, and a stride of 2.\n",
    "2. Conv Layer 2 should have a 4x4 kernel, no padding, and a stride of 2.\n",
    "3. Conv Layer 3 should have a 3x3 kernel, no padding, and a stride of 1.\n",
    "\n",
    "The output of these layers should be a 3x3 image with 16 channels.\n",
    "You should flatten the image (you can use `.view()`, `torch.flatten()`, or `nn.Flatten()`) and then pass it through 2 linear layers:\n",
    "1. Linear Layer 1 should take the flattened image and map it to a vector with 16 features.\n",
    "2. Linear Layer 2 should map its vector to logits.\n",
    "\n",
    "Do not forget to add nonlinearities between the layers (do not add them to the last layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a0_GBKlkQ9GO"
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gynh4o0GQ9GP"
   },
   "source": [
    "Train you convolution network below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AeFfiCH4Q9GP"
   },
   "outputs": [],
   "source": [
    "# TODO: Set seed for reproducibility\n",
    "\n",
    "# TODO: Initalize your ConvNet, called 'conv_net'\n",
    "conv_net = None\n",
    "\n",
    "# TODO: Create an Adam optimizer (lr=.001 works well)\n",
    "\n",
    "# TODO: Train your ConvNet for 2000 steps and set log_val_interval=50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpvG7fl1Q9GP"
   },
   "source": [
    "Now plot your training and validation loss on the same plot and plot your training and validation accuracies on the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mC3GifTWQ9GP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlgZfmwRQ9GP"
   },
   "source": [
    "Create a function that outputs the number of parameters in a network.\n",
    "Remember you can call `.parameters()` to recursively retrieve the `Parameter`s in a `Module`.\n",
    "You could then use `.shape` to figure out the number of parameters in a `Parameter` or you could flatten the parameter and get its length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "INzCWjI5Q9GP"
   },
   "outputs": [],
   "source": [
    "def get_n_net_params(net):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQR4_hPjQ9GQ"
   },
   "source": [
    "Print the number of parameters and the accuracy of your trained `MLP` and `ConvNet` networks and then write down below:\n",
    "- Which one is more accurate?\n",
    "- Which one is smaller?\n",
    "\n",
    "*Note: You can use `validation()` to get the accuracy of your network.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wVCnQ1ccQ9GQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
