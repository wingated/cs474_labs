{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYabkPq6Osas"
   },
   "source": [
    "# Deep Learning Part 1: PyTorch, Autodifferentiation, and Optimization\n",
    "\n",
    "---\n",
    "\n",
    "While these labs can be completed on your own computer using Jupyter, we suggest using Google Colab. Colab provides a standardized virtual environment and allows you to request a GPU, which will let you perform deep learning much faster. If you choose to do these labs on your own computer, you may have to do extra work in getting your CUDA-compatible GPU working and getting the right versions of the packages for the labs.\n",
    "\n",
    "We won't use GPUs for this lab, but it will be vital to speed up training in some of the future labs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7yAMqLfOquC"
   },
   "source": [
    "# Preface: Jupyter Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0a6MTkmnOquF"
   },
   "source": [
    "All of your labs will utilize jupyter notebooks, which are useful tools, but they have one big side effect: hidden states. Jupyter notebooks store the state of an executed cell, which can spell problems if you are not careful. To see what we mean execute cell 1 and cell 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmumQeH9OquF"
   },
   "outputs": [],
   "source": [
    "# cell 1\n",
    "x = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "psSQuB-TOquG"
   },
   "outputs": [],
   "source": [
    "# cell 2\n",
    "x += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7FvNaj-OquH"
   },
   "source": [
    "Now print `x` below and validate that it equals 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PV64iTjhOquH"
   },
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxyZtWTeOquI"
   },
   "source": [
    "Now execute cell 2 again and print `x` below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G8Wpn0YEOquI"
   },
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OlXeevt7OquJ"
   },
   "source": [
    "This time `x` equals 2, which makes sense because we incremented it twice, but if an outsider looked at this code sequentially it would seem weird that x prints out 1 and 2. Since the state is hidden, an outsider can't assume code cells are run in sequential order, or that the cells weren't edited after being run.\n",
    "\n",
    "Jupyter notebooks are great for development: letting you quickly iterate on a program without executing everything sequentially, but at some point you may run into an issue with hidden state.\n",
    "\n",
    "There are two ways to reset the hidden state in this example. The quickest would be to execute cell 1 again which would reset `x` back to 0. Then if you execute cell 2 and print `x` twice with the two statements below, they should both output 1.\n",
    "\n",
    "To fully reset the state, just restart your Jupyter notebook runtime/session. This erases all data in RAM, but keeps anything stored on disk (installs, datasets, etc.). When you disconnect from the session in Colab, it deletes everything except for what is saved in your notebook file: code, text, and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiquMGSKOquL"
   },
   "source": [
    "---\n",
    "\n",
    "# What is PyTorch and Automatic Differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YedNwhuoOquL"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Mwd_l3-OquM"
   },
   "source": [
    "## Part 1: Tensors and Array Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHvGT9ceOquM"
   },
   "source": [
    "Some of you may already be familiar with NumPy or vector math and so this should be a simple and brief overview.\n",
    "\n",
    "PyTorch tries to be as close to NumPy as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdxCQ0gYOquN"
   },
   "source": [
    "### Tensor creation, shapes, and data types\n",
    "\n",
    "Tensors are n-dimensional containers: 0D tensors are scalars, 1D tensors are vectors, and 2D tensors are matrices. We'll often work with tensors with more than 2 dimensions.\n",
    "\n",
    "Let start by creating a simple tensor.\n",
    "Pass in the array `[1, 2]` into the `torch.tensor()` function and store the tensor as variable `v`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hD19cj9tOquN"
   },
   "outputs": [],
   "source": [
    "v = torch.tensor([1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKZqsb2yOquN"
   },
   "source": [
    "Let us look at shape and data type of `v`.\n",
    "You can view the shape of `v` with `v.shape` or calling `v.size()` and you can look at the data type of `v` with `v.dtype`.\n",
    "\n",
    "Print the shape and data type of `v` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FILMXnbmOquO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-ZYtRxPOquO"
   },
   "source": [
    "We see that that the shape of `v.shape = torch.Size([2])` and its dtype is `torch.int64`.\n",
    "\n",
    "Initialize a new tensor `w` with the following array `[[1.0], [2.0]]` and print out its shape and dtype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8CEuvIr9OquO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ij9Ek7qhOquO"
   },
   "source": [
    "This time our tensor has a shape of `torch.Size([2, 1])` and its dtype is `torch.float32`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZ41g6YqOquO"
   },
   "source": [
    "Let's now change `v` to look like `w`.\n",
    "First we will try to make `v` share the same shape as `w`. `v` has 1 dimension and `w` has 2.\n",
    "Both `v` and `w` have a value of 2 in dimension 0 (using 0-based indexing) and `w` has a value of 1 in dimension 1.\n",
    "\n",
    "To add a dimension to `v` we can call the `v.unsqueeze()` function.\n",
    "It takes in a dimension argument (`dim`) specifying where we want to add the dimension.\n",
    "Because `v` is missing its dimension 1, specify `1` as the `dim` argument.\n",
    "\n",
    "*Note: `.unsqueeze()` returns a reshaped tensor and doesn't modify `v`.*\n",
    "\n",
    "Do not modify `v` and assign the unsqueezed tensor to the `answer` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cfzdW8DoOquO"
   },
   "outputs": [],
   "source": [
    "answer = None # replace None with your solution\n",
    "\n",
    "assert answer.shape == w.shape == torch.Size([2, 1])\n",
    "assert v.shape == torch.Size([2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jb8Xg05hOquP"
   },
   "source": [
    "Now let's change the `v`'s data type. This easiest method is to call is the `.float()` function.\n",
    "\n",
    "*Note: Like `.unsqueeze()`, `.float()` returns a new tensor with the specified dtype and doesn't modify `v`*.\n",
    "Do not modify v and assign the float tensor to the `answer` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9B4G9NJ_OquP"
   },
   "outputs": [],
   "source": [
    "answer = None # replace None with your solution\n",
    "\n",
    "assert answer.dtype == w.dtype == torch.float32\n",
    "assert v.dtype == torch.int64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1FoaU2NOquP"
   },
   "source": [
    "Now let's change `w` to look like `v`, but this time let's do it in one line.\n",
    "\n",
    "First we should change `w`'s dimensions so it matches `v`.\n",
    "To remove a dimension we can call the `.squeeze()` function and specify the dimension we want to remove.\n",
    "This will only remove the dimension if it has **value of 1**, if you squeeze a dimension with a value >1, then nothing changes.\n",
    "If you do not specify a dimension, all dimensions with a value of 1 are removed.\n",
    "**It is good coding practice to specify the dimension.**\n",
    "\n",
    "To make `w` share the same dtype as `v` we can use the `.long()` (because a long is a 64-bit integer).\n",
    "\n",
    "Now convert `w` to have the same shape and dtype as `v`, in 1 line of code. Remember `.squeeze()` and `.long()` returns tensors so you can chain the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4fylIEn8OquQ"
   },
   "outputs": [],
   "source": [
    "answer = None # replace None with your solution\n",
    "\n",
    "assert answer.shape == v.shape == torch.Size([2])\n",
    "assert answer.dtype == v.dtype == torch.int64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nseyumWOquQ"
   },
   "source": [
    "While there are many other dtypes, `torch.float32` and `torch.int64` are the main ones you will encounter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXyNW0IhOquQ"
   },
   "source": [
    "There are a few other helpful tensor creation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wugxs7bcOquQ"
   },
   "outputs": [],
   "source": [
    "torch.zeros(size=(3, 3)) # returns a 3x3 tensor filled with 0's\n",
    "torch.ones(2) # returns a tensor with shape (3,) filled with 1's\n",
    "torch.full((3, 1), fill_value=-1) # returns a tensor with shape (3,) filled with -1's\n",
    "torch.rand(size=(2, 2)) # returns a 2x2 tensor filled with (uniform) random floats between 0 and 1\n",
    "torch.randn(size=(2,2)) # returns a 2x2 tensors filled with random floats drawn from a standard normal distribution\n",
    "\n",
    "torch.zeros_like(v) # returns a tensor with the same shape and dtype as v, but filled with 0's.\n",
    "# You could similarly call ones_like(), full_like(), rand_like()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fd4FzZLaOquR"
   },
   "source": [
    "You can also easily create a tensor with a sequence of integers with the `torch.arange()` function. Just like Python's `range()` function you can specify where the sequence should start, end, and the sequence step size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iLiKqVObOquS"
   },
   "outputs": [],
   "source": [
    "torch.arange(6) # returns a tensor with shape (6,) containing a sequence from 0 to 6 (exclusive), i.e. [0, 1, 2, 3, 4, 5]\n",
    "torch.arange(1, 7) # returns a tensor with shape (6,) containing a sequence from 1 to 7 (exclusive), i.e. [1, 2, 3, 4, 5, 6]\n",
    "torch.arange(1, 7, 2) # returns a tensor with shape (6,) containing sequence [1, 3, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7dgX7SlOquS"
   },
   "source": [
    "### Changing shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKGdsuSaOquT"
   },
   "source": [
    "We have explained how you can add or remove dimensions to a tensor with `.unsqueeze()` and `.squeeze()`.\n",
    "But what if we want reshape a tensor?\n",
    "\n",
    "Using the `torch.arange()` function, create a tensor, named `x`, containing a sequence from 0 to 12 (exclusive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vrGpHdRjOquT"
   },
   "outputs": [],
   "source": [
    "x = None # replace None with your solution\n",
    "\n",
    "assert x.equal(torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GKaLp7yOquT"
   },
   "source": [
    "Let's say we want to change our tensor sequence (vector) into a matrix such that: $$\\begin{bmatrix} 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 \\end{bmatrix} \\rightarrow \\begin{bmatrix} 0 & 1 & 2 & 3 \\\\ 4 & 5 & 6 & 7 \\\\ 8 & 9 & 10 & 11 \\end{bmatrix}$$\n",
    "\n",
    "This matrix has 3 rows and 4 columns, i.e. shape=(3,4), so we can call the `.view()` function and specify the shape we want to view sequence of data.\n",
    "**Note: `.view()` returns a tensor*. Do **not** overwrite `x`, just print out `x.view(3, 4)` and validate that we get the desired matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QSATVKvKOquT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqZuUqtYOquT"
   },
   "source": [
    "Now change the shape of `x` so that: $\\begin{bmatrix} 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7& 8 & 9 & 10 & 11 \\end{bmatrix} \\rightarrow \\begin{bmatrix} 0 & 1 & 2 \\\\ 3 & 4 & 5 \\\\ 6 & 7 & 8 \\\\ 9 & 10 & 11 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XkVpIIXzOquT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-x4Pg6wOquT"
   },
   "source": [
    "Now change the shape so that we that 2 matrices that are 3 by 2: $$\\begin{bmatrix} 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 \\end{bmatrix} \\rightarrow \\begin{bmatrix} \\begin{bmatrix} 0 & 1 \\\\ 2 & 3 \\\\ 4 & 5 \\end{bmatrix} \\\\ \\begin{bmatrix} 6 & 7 \\\\ 8 & 9 \\\\ 10 & 11 \\end{bmatrix} \\end{bmatrix}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BGi3-BWgOquT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q00IyV1pOquT"
   },
   "source": [
    "Notice that the memory in all three cases is contiguous, meaning we can count the numbers from left to right and top to bottom, but what if we wanted our tensor to look like:\n",
    "$$\\begin{bmatrix} 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 \\end{bmatrix} \\rightarrow \\begin{bmatrix} 0 & 4 & 8 \\\\ 1 & 5 & 9 \\\\ 2 & 6 & 10 \\\\ 3 & 7 & 11 \\end{bmatrix}$$\n",
    "We can make use of the `.transpose()` function by first making our data sequence into a 3x4 matrix and then calling `.transpose()`:\n",
    "$$\\begin{aligned}\n",
    "\\begin{bmatrix} 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 \\end{bmatrix} \\rightarrow \\begin{bmatrix} 0 & 1 & 2 & 3 \\\\ 4 & 5 & 6 & 7 \\\\ 8 & 9 & 10 & 11 \\end{bmatrix} \\\\\n",
    "\\begin{bmatrix} 0 & 1 & 2 & 3 \\\\ 4 & 5 & 6 & 7 \\\\ 8 & 9 & 10 & 11 \\end{bmatrix}^\\textrm{T} \\rightarrow \\begin{bmatrix} 0 & 4 & 8 \\\\ 1 & 5 & 9 \\\\ 2 & 6 & 10 \\\\ 3 & 7 & 11 \\end{bmatrix}\n",
    "\\end{aligned}$$\n",
    "The `.transpose()` function takes in two arguments, the two dimensions you want to transpose, this is easy in our case since there are only two dimensions (because we only have two dimension you could also use `.T`, which is an alias for `.transpose()` in two dimensions).\n",
    "\n",
    "Use `.view()` and `.transpose()` and validate your new tensor is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-geOJlK7OquU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PkvWYceIOquU"
   },
   "source": [
    "What if we wanted to flatten the above tensor so that:\n",
    "$$\\begin{bmatrix} 0 & 4 & 8 \\\\ 1 & 5 & 9 \\\\ 2 & 6 & 10 \\\\ 3 & 7 & 11 \\end{bmatrix} \\rightarrow \\begin{bmatrix} 0 & 4 & 8 & 1 & 5 & 9 & 2 & 6 & 10 & 3 & 7 & 11 \\end{bmatrix}$$\n",
    "\n",
    "Use the same `.view()` and `.transpose()` functions you used above, but this time tack on `.view(12)` to flatten the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZJHNg9abOquU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqtd-Kq0OquU"
   },
   "source": [
    "You should have run into `RuntimeError: view size is not compatible with input tensor's size and stride ...`.\n",
    "This error was thrown because the transpose messed with the contiguity of our data and `.view()` ensures that our data remains contiguous.\n",
    "If however, we want to enforce our change, we can call `.reshape()`.\n",
    "Go ahead and rerun the same line of code above, but this time replace `.view(12)` with `.reshape(12)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HKbhfWluOquU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5D5KFr-NOquU"
   },
   "source": [
    "`.view()` is faster than `.reshape()`, so you use `.reshape()` only when needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXEamNTXOquV"
   },
   "source": [
    "### Broadcasting and Array Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TxuBiA4HOquV"
   },
   "outputs": [],
   "source": [
    "s = torch.tensor(1)\n",
    "v1 = torch.tensor([1])\n",
    "v2 = torch.tensor([1, 2])\n",
    "v3 = torch.tensor([1, 2, 3])\n",
    "A = torch.tensor([[1, 2],\n",
    "                  [3, 4]])\n",
    "B = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNZjR5CoOquV"
   },
   "source": [
    "Print out the shape of `s`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q06EhiAgOquV"
   },
   "outputs": [],
   "source": [
    "s.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XABSu6xtOquV"
   },
   "source": [
    "The shape of `s` is `torch.Size([])` because it has no dimensions and is therefore a scalar and so in some cases it will operate different from `v1` which is a vector of size 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYysRt9yOquW"
   },
   "source": [
    "### Broadcasting\n",
    "\n",
    "Broadcasting is how array libraries, such as PyTorch and NumPy, perform arithmetic operations on two arrays with different shapes.\n",
    "\n",
    "There are two main rules to remember:\n",
    "1. If one array has fewer dimensions than another, then we **prepend** (e.g. `unsqueeze(0)`) dimensions to the smaller array until both arrays have the same number of dimensions.\n",
    "    ```python\n",
    "    x.shape == (5, 4, 2)\n",
    "    y.shape == (2, )\n",
    "    # After Step 1\n",
    "    x.shape == (5, 4, 2)\n",
    "    y.shape == (1, 1, 2)\n",
    "    ```\n",
    "2. After Step 1, both arrays must have the same value at each dimension must or one of them must have a value of one. Whenever a dimension of 1 is matched to a dimension >1, it duplicates its data to match the size of the other.\n",
    "    ```python\n",
    "    # Example will Fail\n",
    "    x.shape == (1, 4, 2, 9)\n",
    "    y.shape == (1, 1, 5, 9)\n",
    "    Matches:   (T, T, F, T)\n",
    "\n",
    "    # Example will Pass\n",
    "    x.shape == (1, 4, 2, 9)\n",
    "    y.shape == (2, 4, 1, 9)\n",
    "    Matches:   (T, T, T, T)\n",
    "    ```\n",
    "\n",
    "You do not need to implement these rules, they are done automatically, but you need to know them when you are performing operations on two arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4YbP1Ja9OquX"
   },
   "source": [
    "Example 1: Adding `A + s`\n",
    "\n",
    "Because `A.shape == (2, 2)` and `s.shape == ()` then Step 1 prepends 1 to `s` twice so that it has a shape of (1, 1). Then the value of `s` gets copied on the dimensions where it has a value of 1.\n",
    "\n",
    "<!-- $\\begin{aligned}\n",
    "A + s &= \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} + 1 \\\\  \n",
    "\\textrm{After Step 1} \\\\\n",
    "    &= \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} + \\begin{bmatrix} \\begin{bmatrix} 1 \\end{bmatrix} \\end{bmatrix} \\\\\n",
    "\\textrm{After Step 2} \\\\\n",
    "    &= \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} + \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 3 \\\\ 4 & 5 \\end{bmatrix}\n",
    "\\end{aligned}$ -->\n",
    "\n",
    "$\\begin{aligned}\n",
    "A + s &= \\begin{array}{cc} [[1 & 2]\\; \\\\ \\;[3 & 4]] \\end{array} + 1 \\\\  \n",
    "\\textrm{After Step 1} \\\\\n",
    "    &= \\begin{array}{cc} [[1 & 2]\\; \\\\ \\;[3 & 4]] \\end{array} + \\begin{array}{c} [[1]] \\end{array} \\\\  \n",
    "\\textrm{After Step 2} \\\\\n",
    "    &= \\begin{array}{cc} [[1 & 2]\\; \\\\ \\;[3 & 4]] \\end{array} + \\begin{array}{c} [[1 & 1]\\; \\\\ \\;[1 & 1]] \\end{array} = \\begin{array}{cc} [[2 & 3]\\; \\\\ \\;[4 & 5]] \\end{array} \\\\  \n",
    "\\end{aligned}$\n",
    "\n",
    "Add `A + s` below and validate you get the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LL1of2EzOquX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMzL9MG-OquX"
   },
   "source": [
    "*Note, you can get the same result if you add by a float/integer, i.e. `A + 1`. PyTorch works well with floats and integers.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_ezGsdeOquX"
   },
   "source": [
    "Example 2: Adding `B + v3`.\n",
    "\n",
    "Because `B.shape == (2, 3)` and `v3.shape == (3,)` then Step 1 prepends 1 to `v3` once so that it has a shape of (1, 3). Then the value of `v3` gets copied on the dimensions where it has a value of 1.\n",
    "\n",
    "$\\begin{aligned}\n",
    "B + v_3 &= \\begin{array}{ccc} [[1 & 2 & 3]\\; \\\\ \\;[4 & 5 & 6]] \\end{array} + \\begin{array}{ccc} [1 & 2 & 3] \\end{array} \\\\  \n",
    "\\textrm{After Step 1} \\\\\n",
    "    &= \\begin{array}{ccc} [[1 & 2 & 3]\\; \\\\ \\;[4 & 5 & 6]] \\end{array} + \\begin{array}{ccc} [[1 & 2 & 3]] \\end{array} \\\\  \n",
    "\\textrm{After Step 2} \\\\\n",
    "    &= \\begin{array}{ccc} [[1 & 2 & 3]\\; \\\\ \\;[4 & 5 & 6]] \\end{array} + \\begin{array}{ccc} [[1 & 2 & 3]\\; \\\\ \\;[1 & 2 & 3]] \\end{array} = \\begin{array}{ccc} [[2 & 4 & 6]\\; \\\\ \\;[5 & 7 & 9]] \\end{array} \\\\  \n",
    "\\end{aligned}$\n",
    "\n",
    "Add `B + v3` below and validate you get the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rgcK8zzkOquY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NCyeULhuOquY"
   },
   "source": [
    "Example 3: Adding `B + v2`\n",
    "\n",
    "We want to happen is for:\n",
    "\n",
    "$\\begin{aligned}\n",
    "B + v_2 &= \\begin{array}{ccc} [[1 & 2 & 3]\\; \\\\ \\;[4 & 5 & 6]] \\end{array} + \\begin{array}{cc} [1 & 2] \\end{array} \\\\  \n",
    "\\textrm{After Step 1} \\\\\n",
    "    &= \\begin{array}{ccc} [[1 & 2 & 3]\\; \\\\ \\;[4 & 5 & 6]] \\end{array} + \\begin{array}{c} [[1]\\; \\\\ \\;[2]] \\end{array} \\\\  \n",
    "\\textrm{After Step 2} \\\\\n",
    "    &= \\begin{array}{ccc} [[1 & 2 & 3]\\; \\\\ \\;[4 & 5 & 6]] \\end{array} + \\begin{array}{c} [[1 & 1 & 1]\\; \\\\ \\;[2 & 2 & 2]] \\end{array} = \\begin{array}{ccc} [[2 & 3 & 4]\\; \\\\ \\;[6 & 7 & 8]] \\end{array}\n",
    "\\end{aligned}$\n",
    "\n",
    "Try by first running `B + v2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XPDrm-lYOquY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iabDzcLwOquY"
   },
   "source": [
    "This does not work.\n",
    "You should have run into the following error `RuntimeError: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1.`\n",
    "\n",
    "Let's quickly parse this statement. `The size of tensor a`, i.e. `B`, `must match the size of tensor b`, i.e. `v2`, `at dimension 1`.\n",
    "`B.shape == (2, 3)` and `v2.shape == (2,)` and after Step 1 `v2.shape == (1, 2)`.\n",
    "Therefore, the value at `B`'s 1st dimension is `(3)` and the value at `v2`'s 1st dimension is `(2)`. Hence the error.\n",
    "\n",
    "We need to use an `.unsqueeze()` to fix the dimensions of `v2`. Fix the dimensions of `v2` below and validate you get the correct answer: $\\begin{array}{ccc} [[2 & 3 & 4]\\; \\\\ \\;[6 & 7 & 8]] \\end{array}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jn4VahMbOquY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQT7kSaWOquZ"
   },
   "source": [
    "### Multiplication Notation\n",
    "$A \\times x$ versus $Ax$\n",
    "\n",
    "When we use the notation $A \\times x$ we mean we are performing elementwise multiplication, and in Python it looks like `A * x`, but if we use notation $Ax$ we mean that we are performing matrix multiplication and in Python it looks like `A @ x`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_wRvZXaOquZ"
   },
   "source": [
    "### Broadcasting Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6AiBkGtkOquZ"
   },
   "outputs": [],
   "source": [
    "a = torch.arange(2)\n",
    "b = torch.arange(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1gvO9idOquZ"
   },
   "source": [
    "Add `a` and `b` such that: $$\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} + \\begin{bmatrix} 0 & 1 & 2 & 3 \\end{bmatrix} = \\begin{bmatrix}  0 & 1 & 2 & 3 \\\\ 1 & 2 & 3 & 4 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J-MZDqB3Oqua"
   },
   "outputs": [],
   "source": [
    "answer = None # replace None with your solution\n",
    "\n",
    "assert torch.equal(answer, torch.tensor([[0, 1 , 2, 3], [1, 2, 3, 4]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9h9VXLLOqua"
   },
   "source": [
    "Change `b` into a 2x2 matrix (use `.view()`) multiply with `a` along `b`'s column dimension. $$\\begin{bmatrix} 0 & 1 \\\\ 2 & 3 \\end{bmatrix} * \\begin{bmatrix} 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 0 & 1 \\\\ 0 & 3 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S5UwpLA7Oqua"
   },
   "outputs": [],
   "source": [
    "answer  = None # replace None with your solution\n",
    "\n",
    "assert torch.equal(answer, torch.tensor([[0, 1], [0, 3]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Um_34FbKOqua"
   },
   "source": [
    "Change `b` into a 2x2 matrix (use `.view()`) multiply with `a` along `b`'s row dimension. $$\\begin{bmatrix} 0 & 1 \\\\ 2 & 3 \\end{bmatrix} * \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 0 & 0 \\\\ 2 & 3 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jr99bcDnOqua"
   },
   "outputs": [],
   "source": [
    "answer = None # replace None with your solution\n",
    "\n",
    "assert torch.equal(answer, torch.tensor([[0, 0], [2, 3]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rQjWhvBOqua"
   },
   "source": [
    "Change `b` into a 2x2 matrix and matrix multiply `a` such that:  $$\\begin{bmatrix} 0 & 1 \\\\ 2 & 3 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rfapMwiYOqua"
   },
   "outputs": [],
   "source": [
    "answer = None # replace None with your solution\n",
    "\n",
    "assert torch.equal(answer, torch.tensor([[1], [3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "id5lWBlAOqua"
   },
   "outputs": [],
   "source": [
    "c = torch.zeros((1, 2))\n",
    "d = torch.zeros(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvzBZBBmOqua"
   },
   "source": [
    "Add `c` and `d` such that the resulting shape is (1, 3, 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YsaJmHhuOqua"
   },
   "outputs": [],
   "source": [
    "answer = None # replace None with your solution\n",
    "\n",
    "assert answer.shape == torch.Size([1, 3, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLYejjkzOqub"
   },
   "source": [
    "---\n",
    "\n",
    "# Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2OflsKQLOqub"
   },
   "source": [
    "PyTorch is a machine learning library that provides us with the ability to automatically differentiate (autodiff) functions. In this section we will explore how we can make use of PyTorch's autodiff functions.\n",
    "\n",
    "To help us understand a little more about how PyTorch performs autodiff, we are going to import the `make_dot()` function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mtZIZnxhOquc"
   },
   "outputs": [],
   "source": [
    "!pip install torchviz # `!` signifies a bash operation rather than Python\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hjbjg9omOquc"
   },
   "source": [
    "Let's make scalar tensor `x = torch.tensor(2)` to see how PyTorch works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CeWaetIbOquc"
   },
   "outputs": [],
   "source": [
    "x = torch.tensor(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9SHyEUtOquc"
   },
   "source": [
    "We have already talked about two important properties that a tensor has: `.shape` and `.dtype`.\n",
    "There are three more properties of a tensor we need to introduce: `.requires_grad`, `.grad`, and `.data`.\n",
    "Let's first look at whether `x` is using autograd by printing out `x.requires_grad` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ANbn1k5QOqud"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bFMR8WZlOqud"
   },
   "source": [
    "By default tensors have `x.requires_grad` set to `False`, which means we will never calculate `x`'s gradients. Let's change that by setting `x.requires_grad` to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bx2jeA_LOqud"
   },
   "outputs": [],
   "source": [
    "x.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4_TNPhZOqud"
   },
   "source": [
    "You should run into `RuntimeError: only Tensors of floating point and complex dtype can require gradients.`\n",
    "We need to make `x` into a `torch.float32` tensor.\n",
    "Go ahead and call the `.float()` function (remember to update `x` with the output of `.float()`) and then set `x.requires_grad` to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8XMMaB_iOqud"
   },
   "outputs": [],
   "source": [
    "# convert x to float and set requires_grad to True\n",
    "\n",
    "assert x.requires_grad == True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_29hn0FOque"
   },
   "source": [
    "Now print out `x.grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wUxuvcPhOque"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ADJg2DyoOque"
   },
   "source": [
    "`x` has no gradients right now because we haven't computed any derivatives.\n",
    "Let's fix that, create a simple function $y = 3x$, i.e. `y = 3*x`, and print out the value of `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m3V5-IcpOque"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V9Q6au3dOque"
   },
   "source": [
    "Because $x = 2$ the output of $y = 3(2) = 6$.\n",
    "More than that though, there is a `grad_fn=<MulBackward0>` attached to our tensor.\n",
    "This gradient function is part of a computation graph, which is the history of operations that were necessary to compute `y`.\n",
    "PyTorch utilizes this computation graph to know how to compute the gradients of all tensors involved in making `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k15Zqf4BOque"
   },
   "source": [
    "To visualize this computation graph, let's use the `make_dot()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8WXaj5jsOquf"
   },
   "outputs": [],
   "source": [
    "make_dot(y, params={\"x\": x, \"y\": y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FyHEL0puOquf"
   },
   "source": [
    "Our computation graph starts with `x` fed into `AccumulatedGrad` (which we will discuss later), that is fed into `MulBackward0` which was the `grad_fn` attached to tensor `y`.\n",
    "`x` and `y` both have `()` underneath them specifying the shape of the tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62mxG-xqOquf"
   },
   "source": [
    "Let's calculate the derivative of our function by calling `y.backward()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-nZKw9j-Oquf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYusyP1OOqug"
   },
   "source": [
    "We know that the derivative of our function $\\frac{dy}{dx} = \\frac{d}{dx} 3x = 3$. Print out `x.grad` to validate that PyTorch is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6m5JUIvtOquh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_vModmQOquh"
   },
   "source": [
    "What happens if we call `y.backward()` again?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5GqqRi8Oqui"
   },
   "outputs": [],
   "source": [
    "# call y.backward() again\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8aZIebkoOqui"
   },
   "source": [
    "You should run into a `RuntimeError: Trying to backward through the graph a second time ...` because PyTorch frees up certain resources required to compute gradients once `.backward()` is called.\n",
    "You can call `y.backward()` multiple times if you always specify `y.backward(retain_graph=True)`, but you should never do that in this class.\n",
    "If you run into this error hereafter, you most likely made a mistake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfPtCAKDOquj"
   },
   "source": [
    "Let's compute the derivative of $y = 3x$ again.\n",
    "This time compute `y = 3*x` first and then call `y.backward()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5VJoC2atOquj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GuIZWe0LOquj"
   },
   "source": [
    "Now print out `x.grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AaLApCvAOquj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4mMJAs_Oquk"
   },
   "source": [
    "**6?** Wasn't the derivative of $3x$ equal to 3? This occurred because PyTorch accumulates the gradients every time you call `.backward()`. Because `x.grad` was previously 3 and this new gradient is 3, then `x.grad` is now equal to 6.\n",
    "This is what `AccumulatedGrad` meant on the computation graph.\n",
    "To avoid your gradients from accumulating set `x.grad = None` (we call this zeroing out the gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cjq_hMW_Oquk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4v-1jmUOquk"
   },
   "source": [
    "Now set `y = 3*x`, call `y.backward()`, and print out `x.grad` and validate that it equals 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jArU10_COquk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2ZsLY0VOqul"
   },
   "source": [
    "What about $x^2$? Set $y = x^2$, i.e. `y = x**2` and print out `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mFnct0ITOqul"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQgyQnz0Oqul"
   },
   "source": [
    "Because `x = 2` the output of $y = (2)^2 = 4$ and as before we have a `grad_fn=<PowBackward0>` attached to our tensor.\n",
    "\n",
    "Now let's visualize the computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vfjybVclOqul"
   },
   "outputs": [],
   "source": [
    "make_dot(y, params={\"y\": y, \"x\": x}, show_attrs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFyy3fWiOqum"
   },
   "source": [
    "We specified `show_attrs=True` this time and we see in `PowBackward0` the exponent that was used stored in the computation graph.\n",
    "\n",
    "Let's calculate the derivative of of our function by calling `y.backward()` and don't forget to zero out your gradient first, i.e. `x.grad = None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pCbQA0v7Oqum"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdllcOLFOqum"
   },
   "source": [
    "We know that the derivative of our function $\\frac{dy}{dx} = \\frac{d}{dx} x^2 = 2x = 2(2) = 4$, print out `x.grad` to validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ssbw-m_yOqum"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMpRkApcOqum"
   },
   "source": [
    "Now let's calculate a multivariable derivative $a^2 (x+1)^2$.\n",
    "\n",
    "Create a scalar tensor `a` with a value of 2 and a scalar tensor `x` with a value of 3.\n",
    "Make sure both have `.requires_grad` set to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BQeX6yDxOqum"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHt24ZOUOqun"
   },
   "source": [
    "Now set $y = a^2(x+1)^2$ and print out `y`. Validate that $y = a^2(x+1)^2 = (2)^2 * ((3) + 1)^2 = 4 * 4^2 = 64$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TpiYuTO6Oqun"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTXm2v0AOqun"
   },
   "source": [
    "Now let's look at the computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7s_5hXbDOqun"
   },
   "outputs": [],
   "source": [
    "make_dot(y, params={\"a\": a, \"x\": x, \"y\": y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yz7GFzLsOquo"
   },
   "source": [
    "Now call `y.backward()` and validate that\n",
    "\n",
    "$\\frac{dy}{dx} = \\frac{d}{dx} a^2(x+1)^2 = 2a^2(x+1) = 2(2)^2(3 + 1) = 32$\n",
    "\n",
    "$\\frac{dy}{da} = \\frac{d}{da} a^2(x+1)^2 = 2a(x+1)^2 = 2(2)(3 + 1)^2 = 64$\n",
    "\n",
    "by printing `x.grad` and `a.grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5gvfw3FDOquo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GY0Y3O7MOqup"
   },
   "source": [
    "Hopefully, it is clear that there all all kinds of functions you could put in that can be differentiated by PyTorch.\n",
    "Let's now work with vector and matrix differentiation.\n",
    "\n",
    "Define two new tensors:\n",
    "- `A` with data `[[0, 1, 2], [3, 4, 5]]` with `requires_grad = True`\n",
    "- `x` with data `[-1, 0, 1]` with `requires_grad = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6DfRbIbOqup"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lafdsEDrOqup"
   },
   "source": [
    "Now set $y = (Ax)^2$ and print out `y`. Validate that the output $y = (Ax)^2 = \\left( \\begin{bmatrix} 0 & 1 & 2 \\\\ 3 & 4 & 5 \\end{bmatrix} \\begin{bmatrix} -1 \\\\ 0 \\\\ 1 \\end{bmatrix} \\right)^2 = \\left( \\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix} \\right)^2 = \\begin{bmatrix} 2^2 \\\\ 2^2 \\end{bmatrix} = \\begin{bmatrix} 4 \\\\ 4 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-fnnMcYnOquq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "By-pWDfcOquq"
   },
   "source": [
    "Now the \"derivative\" we are computing here is called the Jacobian, because $y$ is a vector-valued function. The Jacobian contains the derivative of every output, $y_i$, with respect to every input, $x_j$. Therefore, the Jacobian of $y$ with respect to $x$ is:\n",
    "\n",
    "$\\begin{aligned}\n",
    "J_y = \\begin{bmatrix} \\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2} & \\frac{\\partial y_1}{\\partial x_3} \\\\ \\frac{\\partial y_2}{\\partial x_1} & \\frac{\\partial y_2}{\\partial x_2} & \\frac{\\partial y_2}{\\partial x_3} \\end{bmatrix} = \\frac{\\partial y}{\\partial x} &= \\frac{\\partial}{\\partial x} \\left( \\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} \\right)^2 \\\\\n",
    "    &= \\frac{\\partial}{\\partial x} \\left( \\begin{bmatrix} A_{11}x_1 + A_{12}x_2 + A_{13}x_3 \\\\ A_{21}x_1 + A_{22}x_2 + A_{23}x_3 \\end{bmatrix} \\right)^2 \\\\\n",
    "    &= \\frac{\\partial}{\\partial x} \\begin{bmatrix} (A_{11}x_1 + A_{12}x_2 + A_{13}x_3)^2 \\\\ (A_{21}x_1 + A_{22}x_2 + A_{23}x_3)^2 \\end{bmatrix} \\\\\n",
    "    &= \\begin{bmatrix} \\frac{\\partial}{\\partial x} (A_{11}x_1 + A_{12}x_2 + A_{13}x_3)^2 \\\\ \\frac{\\partial}{\\partial x} (A_{21}x_1 + A_{22}x_2 + A_{23}x_3)^2 \\end{bmatrix} \\\\\n",
    "    &= \\begin{bmatrix} \\frac{\\partial}{\\partial x_1} (A_{11}x_1 + A_{12}x_2 + A_{13}x_3)^2 & \\frac{\\partial}{\\partial x_2} (A_{11}x_1 + A_{12}x_2 + A_{13}x_3)^2 & \\frac{\\partial}{\\partial x_3} (A_{11}x_1 + A_{12}x_2 + A_{13}x_3)^2  \\\\ \\frac{\\partial}{\\partial x_1} (A_{21}x_1 + A_{22}x_2 + A_{23}x_3)^2 & \\frac{\\partial}{\\partial x_2} (A_{21}x_1 + A_{22}x_2 + A_{23}x_3)^2 & \\frac{\\partial}{\\partial x_3} (A_{21}x_1 + A_{22}x_2 + A_{23}x_3)^2 \\end{bmatrix} \\\\\n",
    "    &= \\begin{bmatrix} 2A_{11}(A_{11}x_1 + A_{12}x_2 + A_{13}x_3) & 2A_{12}(A_{11}x_1 + A_{12}x_2 + A_{13}x_3) & 2A_{13}(A_{11}x_1 + A_{12}x_2 + A_{13}x_3)  \\\\ 2A_{21}(A_{21}x_1 + A_{22}x_2 + A_{23}x_3) & 2A_{22}(A_{21}x_1 + A_{22}x_2 + A_{23}x_3) & 2A_{23}(A_{21}x_1 + A_{22}x_2 + A_{23}x_3) \\end{bmatrix} \\\\\n",
    "    &= \\begin{bmatrix} 0 & 4 & 8  \\\\ 12 & 16 & 20 \\end{bmatrix}\n",
    "\\end{aligned}$\n",
    "\n",
    "Now call `y.backward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7KtI2BIYOquq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4Lg1XYvOqur"
   },
   "source": [
    "You should run into `RuntimeError: grad can be implicitly created only for scalar outputs`.\n",
    "We can only call `.backward()` on scalar tensors in pytorch; there are some detailed explanations on why online, but simple reason behind this is because backpropagating on a scalar reduces the dimensionality of our gradients making backpropagation cheaper and quicker.\n",
    "Which makes sense because `x` is a vector and the Jacobian we calculated is a matrix.\n",
    "\n",
    "We can easily circumvent this issue by summing up our `y` tensor. Set `y_sum = torch.sum(y)` and call `.backward()` on `y_sum`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HLsGYnkgOqur"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bjb6bGI4Oqur"
   },
   "source": [
    "Now let's look at the gradients from our new function.\n",
    "\n",
    "$\\begin{aligned}\n",
    "\\frac{\\partial y}{\\partial x} &= \\frac{\\partial}{\\partial x} \\sum \\left( \\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} \\right)^2 \\\\\n",
    "    &= \\sum \\frac{\\partial}{\\partial x} \\left( \\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} \\right)^2 \\\\\n",
    "    &= \\sum \\frac{\\partial}{\\partial x} \\left( \\begin{bmatrix} A_{11}x_1 + A_{12}x_2 + A_{13}x_3 \\\\ A_{21}x_1 + A_{22}x_2 + A_{23}x_3 \\end{bmatrix} \\right)^2 \\\\\n",
    "    & \\quad \\quad \\vdots \\\\\n",
    "    &= \\sum \\begin{bmatrix} 2A_{11}(A_{11}x_1 + A_{12}x_2 + A_{13}x_3) & 2A_{12}(A_{11}x_1 + A_{12}x_2 + A_{13}x_3) & 2A_{13}(A_{11}x_1 + A_{12}x_2 + A_{13}x_3)  \\\\ 2A_{21}(A_{21}x_1 + A_{22}x_2 + A_{23}x_3) & 2A_{22}(A_{21}x_1 + A_{22}x_2 + A_{23}x_3) & 2A_{23}(A_{21}x_1 + A_{22}x_2 + A_{23}x_3) \\end{bmatrix} \\\\\n",
    "    &= \\sum \\begin{bmatrix} 0 & 4 & 8  \\\\ 12 & 16 & 20 \\end{bmatrix} \\\\\n",
    "    &= \\begin{bmatrix} 12 & 20 & 28  \\end{bmatrix} \\\\\n",
    "\\end{aligned}$\n",
    "\n",
    "**Note that by summing the output of `y` we ended up summing our Jacobian**.\n",
    "\n",
    "I will quickly show $\\frac{\\partial y}{\\partial A}$ in abbreviated form:\n",
    "\n",
    "$\\begin{aligned}\n",
    "\\frac{\\partial y}{\\partial A} &= \\frac{\\partial}{\\partial x} \\sum \\left( Ax \\right)^2 \\\\\n",
    "    &= \\frac{\\partial}{\\partial A}  (A_1x)^2 + (A_2x)^2 \\\\\n",
    "    &= \\begin{bmatrix} \\frac{\\partial}{\\partial A_1} (A_1x)^2 + (A_2x)^2 \\\\ \\frac{\\partial}{\\partial A_2} (A_1x)^2 + (A_2x)^2  \\end{bmatrix} \\\\\n",
    "    &= \\begin{bmatrix} 2(A_1x)x^T \\\\ 2(A_2x)x^T  \\end{bmatrix} \\\\\n",
    "    &= \\begin{bmatrix} 4x^T \\\\ 4x^T  \\end{bmatrix} \\\\\n",
    "    &= \\begin{bmatrix} -4 & 0 & 4 \\\\ -4 & 0 & 4  \\end{bmatrix} \\\\\n",
    "\\end{aligned}$\n",
    "\n",
    "Print out `x.grad` and `A.grad` and validate you got the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eFn__s7xOqur"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NR2bfXm-Oqus"
   },
   "source": [
    "And just for fun, let's look at the computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mCl0KiLpOqus"
   },
   "outputs": [],
   "source": [
    "make_dot(y_sum, params={\"A\": A, \"x\": x, \"y_sum\": y_sum})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHtkfp91Oqus"
   },
   "source": [
    "## Final Notes about Autodiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juaxEdn8Oqus"
   },
   "source": [
    "There are two other functions that are useful when using autodifferentiation: `.clone()` and `.detach()`.\n",
    "\n",
    "- `.clone()` gives us the ability to copy a tensor and remain on the computation graph.\n",
    "- `.detach()` gives us the ability to copy a tensor, but to become detached from computation graph, i.e. the copy will have `requires_grad = False`.\n",
    "\n",
    "To illustrate this we will initialize two tensors: $x=1$ and $y=2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ofw7LLdsOqut"
   },
   "outputs": [],
   "source": [
    "x = torch.tensor(1).float()\n",
    "y = torch.tensor(2).float()\n",
    "\n",
    "x.requires_grad = True\n",
    "y.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1A6Hx_fOqut"
   },
   "source": [
    "And vizualize the computation graph of:\n",
    "```python\n",
    "z = x + y\n",
    "l = 2*z + 3*z\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hhv1_fzUOqut"
   },
   "outputs": [],
   "source": [
    "z = x + y\n",
    "l = 2*z + 3*z\n",
    "\n",
    "make_dot(l, params={\"x\": x, \"y\": y, \"l\": l})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4076oW3wOqut"
   },
   "source": [
    "The output of `l` is $l = 2z + 3z = 5(x + y) = 15$.\n",
    "The gradients of $x$ and $y$ are then $\\frac{d}{dx} 5(x+y) = \\frac{d}{dx} 5x = 5$ and $\\frac{d}{dy} 5(x+y) = \\frac{d}{dy} 5y = 5$. Take the derivative of `l` and print out `l` and the gradients of `x` and `y` to validate this is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZqbO_c1wOquu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1-5fDaIOquu"
   },
   "source": [
    "### `.clone()`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UneExL8VOquu"
   },
   "source": [
    "Now compute the same function, but this time let `l = 2*z + 3*z.clone()`. Call `make_dot()` and visualize the computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xAL_laJhOquv"
   },
   "outputs": [],
   "source": [
    "# compute z and l\n",
    "\n",
    "make_dot(l, params={\"x\": x, \"y\": y, \"l\": l})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXq9ll_5Oquv"
   },
   "source": [
    "The computation graph now has `CloneBackward0` attached to, but since `.clone()` doesn't change any gradients then the output of `l` and `x.grad` and `y.grad` should all be the same as above.\n",
    "Take the derivative of `l` (don't forget to zero out the gradients of `x` and `y`) and print out `l` and the gradients of `x` and `y` to validate this is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DKB_z8tEOquv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JZxBdVFOquv"
   },
   "source": [
    "### `.detach()`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DikCUy2UOquv"
   },
   "source": [
    "Once again compute the same function, but this time let `l = 2*z + 3*z.detach()`. Call `make_dot()` and visualize the computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2deQc6mxOquv"
   },
   "outputs": [],
   "source": [
    "# compute z and l\n",
    "\n",
    "make_dot(l, params={\"x\": x, \"y\": y, \"l\": l})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TnqIWAIOquv"
   },
   "source": [
    "The computation graph has changed because `3*z.detach()` looks like the number 9 to the compuation graph.\n",
    "The output has not changed though, $l = 2z + 9 = 2(x + y) + 9 = 6 + 9 = 15$. However, our gradients are changed because $\\frac{d}{dx} 2(x+y) + 9 = \\frac{d}{dx} 2x = 2$ and $\\frac{d}{dy} 2(x+y) + 9= \\frac{d}{dy} 2y = 2$.\n",
    "Take the derivative of `l` (don't forget to zero out the gradients of `x` and `y`) and print out `l` and the gradients of `x` and `y` to validate this is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MYTFhlQaOquw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gH4C-ixiOquw"
   },
   "source": [
    "---\n",
    "\n",
    "# Gradient Descent Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bgKzMWRTOquw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib.animation as animation\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iAXxMZaWOquw"
   },
   "source": [
    "Why do we care about using PyTorch's autodiff functionality?\n",
    "Because it can be utilized for optimization.\n",
    "Assume for example, we have $n$ data points and we want to find a line that best fits the data, differentiation can tell us how to find the best fitting line through gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcbyZgD-Oquw"
   },
   "source": [
    "First let's create a Random Number Generator (RNG) to make sure our experiment is reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fm668cwkOqux"
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qm5pRfgOquy"
   },
   "source": [
    "And now let's generate our data.\n",
    "The true slope to our line is $m=.3$ and the our true intercept is $b=.25$.\n",
    "Our data is 9 data points along this line with some added noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GslnNWE0Oquy"
   },
   "outputs": [],
   "source": [
    "true_m = .3\n",
    "true_b = .25\n",
    "x_data = np.linspace(.1, .9, 9)\n",
    "noise = rng.normal(0, 1, size=x_data.shape) * .1\n",
    "y_data = true_m*x_data + true_b + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rx8GpfbCOquy"
   },
   "source": [
    "Below are some helper functions.\n",
    "You don't need to look at them, just execute the cell below and move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "La6b6KBGOquy"
   },
   "outputs": [],
   "source": [
    "def _add_true_fn_and_data(ax, handles, labels):\n",
    "    ax.set_xlim(-.05, 1.05)\n",
    "    ax.set_ylim(-.05, 1.05)\n",
    "    ax.set_xlabel(r\"x\")\n",
    "    ax.set_ylabel(r\"y\")\n",
    "    ax.set_title(\"Input/Output Plot\")\n",
    "    # Plot data\n",
    "    ax.scatter(x_data, y_data)\n",
    "    # Plot true function\n",
    "    xs = np.linspace(0, 1, 100)\n",
    "    true_ys = true_m*xs + true_b\n",
    "    handle, = ax.plot(xs, true_ys, label=\"true fn\")\n",
    "    handles.append(handle)\n",
    "    labels.append(\"true fn\")\n",
    "\n",
    "\n",
    "def _add_pred_fn_and_errors(pred_m, pred_b, ax, handles, labels):\n",
    "    xs = np.linspace(0, 1, 100)\n",
    "    pred_ys = pred_m * xs + pred_b\n",
    "    pred_line, = ax.plot(xs, pred_ys, label=\"pred fn\")\n",
    "    handles.append(pred_line)\n",
    "    labels.append(\"pred fn\")\n",
    "\n",
    "    pred_y = pred_m * x_data + pred_b\n",
    "    errors = pred_y - y_data\n",
    "    error_lines = []\n",
    "    for x_i, y_i, error in zip(x_data, y_data, errors):\n",
    "        error_line, = ax.plot([x_i, x_i], [y_i, y_i + error], c=\"red\", label=\"error\")\n",
    "        error_lines.append(error_line)\n",
    "    handles.append(error_line)\n",
    "    labels.append(\"error\")\n",
    "    return pred_line, error_lines\n",
    "\n",
    "\n",
    "def _add_loss_surface(ax, m_min=-1, m_max=1.5, b_min=-.4, b_max=.9):\n",
    "    steps = 50\n",
    "    m_space = np.linspace(m_min, m_max, steps)\n",
    "    b_space = np.linspace(b_min, b_max, steps)\n",
    "\n",
    "    MM, BB = np.meshgrid(m_space, b_space, indexing=\"ij\")\n",
    "    MM = MM.reshape(steps * steps)\n",
    "    BB = BB.reshape(steps * steps)\n",
    "\n",
    "    YY = MM[:, None] @ x_data[None, :] + BB[:, None]\n",
    "    EE = YY - y_data[None, :]\n",
    "    EE = np.mean(EE**2, axis=1)\n",
    "\n",
    "    MM = MM.reshape(steps, steps)\n",
    "    BB = BB.reshape(steps, steps)\n",
    "    EE = EE.reshape(steps, steps)\n",
    "\n",
    "    ax.contourf(MM, BB, np.log2(EE))\n",
    "    ax.set_title(\"Loss Surface\")\n",
    "    ax.set_xlabel(\"slope \" + r\"($m$)\")\n",
    "    ax.set_ylabel(\"intercept \" + r\"($b$)\")\n",
    "\n",
    "\n",
    "def plot_data(pred_m=None, pred_b=None, figsize=(6, 6)):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    handles = []\n",
    "    labels = []\n",
    "\n",
    "    _add_true_fn_and_data(ax=ax, handles=handles, labels=labels)\n",
    "\n",
    "    # Plot predicted function\n",
    "    if pred_m is not None or pred_b is not None:\n",
    "        if pred_m is None:\n",
    "            pred_m = 0\n",
    "        if pred_b is None:\n",
    "            pred_b = 0\n",
    "        _add_pred_fn_and_errors(pred_m=pred_m, pred_b=pred_b, ax=ax, handles=handles, labels=labels)\n",
    "\n",
    "    ax.legend(handles, labels)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_1d_loss_surface(pred_ms=None, pred_bs=None, figsize=(12, 6)):\n",
    "    if pred_ms is None and pred_bs is None:\n",
    "        raise ValueError(f\"Expected pred_ms or pred_bs to be not None\")\n",
    "    if pred_ms is not None and pred_bs is not None:\n",
    "        print(f\"Both pred_ms and pred_bs are not None and function will default to using pred_ms\")\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "    handles = []\n",
    "    labels = []\n",
    "\n",
    "    ### Input Output ###\n",
    "    inp_out_ax = axes[0]\n",
    "    _add_true_fn_and_data(ax=inp_out_ax, handles=handles, labels=labels)\n",
    "\n",
    "    # Plot predicted function\n",
    "    pred_vars = None\n",
    "    if pred_ms is not None:\n",
    "        pred_ms = np.array(pred_ms)\n",
    "        pred_ms = np.concatenate((pred_ms, pred_ms[::-1]))\n",
    "        pred_bs = np.zeros_like(pred_ms)\n",
    "        loss_title = \"Loss Surface With Respect To Slope\"\n",
    "        loss_xlabel = \"slope\"\n",
    "        pred_vars = pred_ms\n",
    "    else:\n",
    "        pred_bs = np.array(pred_bs)\n",
    "        pred_bs = np.concatenate((pred_bs, pred_bs[::-1]))\n",
    "        pred_ms = np.zeros_like(pred_bs)\n",
    "        loss_title = \"Loss Surface With Respect To Intercept\"\n",
    "        loss_xlabel = \"intercept\"\n",
    "        pred_vars = pred_bs\n",
    "\n",
    "    pred_line, error_lines = _add_pred_fn_and_errors(pred_m=pred_ms[0], pred_b=pred_bs[0], ax=inp_out_ax, handles=handles, labels=labels)\n",
    "\n",
    "    inp_out_ax.legend(handles, labels)\n",
    "\n",
    "    ### 1D Loss Surface ###\n",
    "    loss_ax = axes[1]\n",
    "\n",
    "    pred_ys = x_data[:, None] @ pred_ms[None, :] + pred_bs\n",
    "    sses = np.sum((y_data[:, None] - pred_ys)**2, axis=0)\n",
    "\n",
    "    loss_ax.plot(pred_vars[:len(pred_vars)//2], sses[:len(sses)//2])\n",
    "\n",
    "    loss_scatter = loss_ax.scatter(pred_vars[0:1], sses[0:1])\n",
    "\n",
    "    loss_ax.set_title(loss_title)\n",
    "    loss_ax.set_xlabel(loss_xlabel)\n",
    "    loss_ax.set_ylabel(\"Sum Square Error\")\n",
    "\n",
    "    xs = np.linspace(0, 1, 100)\n",
    "    def update(frame):\n",
    "        # for each frame, update the data stored on each artist.\n",
    "        pred_m = pred_ms[frame]\n",
    "        pred_b = pred_bs[frame]\n",
    "        pred_ys = pred_m * xs + pred_b\n",
    "\n",
    "        pred_line.set_ydata(pred_ys)\n",
    "\n",
    "        pred_y = pred_m * x_data + pred_b\n",
    "        errors = pred_y - y_data\n",
    "\n",
    "        for error_line, y_i, error in zip(error_lines, y_data, errors):\n",
    "            error_line.set_ydata([y_i, y_i + error])\n",
    "\n",
    "        loss_scatter.set_offsets(np.array([pred_vars[frame], sses[frame]]))\n",
    "\n",
    "        return (pred_line, *error_lines, loss_scatter)\n",
    "\n",
    "    anim = animation.FuncAnimation(fig=fig, func=update, frames=len(pred_ms), interval=50, blit=True)\n",
    "    from IPython.display import HTML\n",
    "    anim = HTML(anim.to_html5_video())\n",
    "    plt.close('all')\n",
    "    del fig\n",
    "    return anim\n",
    "\n",
    "\n",
    "def plot_2d_loss_surface():\n",
    "    def fan_linspace(start, diff, step, fan_up=True):\n",
    "        quarter_step = step//4\n",
    "        offset = diff\n",
    "        if not fan_up:\n",
    "            offset = -diff\n",
    "        return np.concatenate([np.linspace(start, start+offset, quarter_step),\n",
    "                            np.linspace(start+offset, start, quarter_step),\n",
    "                            np.linspace(start, start-offset, quarter_step),\n",
    "                            np.linspace(start-offset, start, quarter_step)])\n",
    "\n",
    "    sim_steps = 50\n",
    "    anim_pred_ms = np.concatenate([np.full(sim_steps, true_m), fan_linspace(true_m, true_m*2, sim_steps), (true_m - .5 + np.cos(np.linspace(0, 2*np.pi, sim_steps)) / 2).tolist(), (true_m + .5 + np.cos(np.linspace(np.pi, 3*np.pi, sim_steps)) / 2).tolist()])[:, None]\n",
    "    anim_pred_bs = np.concatenate([fan_linspace(true_b, true_b*2, sim_steps), np.full(sim_steps, true_b), (true_b + np.sin(np.linspace(0, 2*np.pi, sim_steps)) / 2).tolist(), (true_b + np.sin(np.linspace(0, 2*np.pi, sim_steps)) / 2).tolist()])[:, None]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    handles = []\n",
    "    labels = []\n",
    "\n",
    "    ### Input Output ###\n",
    "    inp_out_ax = axes[0]\n",
    "    _add_true_fn_and_data(inp_out_ax, handles, labels)\n",
    "    pred_line, error_lines = _add_pred_fn_and_errors(anim_pred_ms[0], anim_pred_bs[0], inp_out_ax, handles, labels)\n",
    "    inp_out_ax.set_xlim(-.2, 1.2)\n",
    "    inp_out_ax.set_ylim(-.2, 1.2)\n",
    "    inp_out_ax.legend(handles, labels)\n",
    "\n",
    "    ### Loss Surface ###\n",
    "    loss_ax = axes[1]\n",
    "    _add_loss_surface(loss_ax)\n",
    "    loss_scatter = loss_ax.scatter(anim_pred_ms[0:1], anim_pred_bs[0:1], c=\"orange\")\n",
    "\n",
    "    xs = np.linspace(0, 1, 100)\n",
    "    def update(frame):\n",
    "        # for each frame, update the data stored on each artist.\n",
    "        pred_m = anim_pred_ms[frame]\n",
    "        pred_b = anim_pred_bs[frame]\n",
    "        pred_ys = pred_m * xs + pred_b\n",
    "\n",
    "        pred_line.set_ydata(pred_ys)\n",
    "\n",
    "        pred_y = pred_m * x_data + pred_b\n",
    "        errors = pred_y - y_data\n",
    "\n",
    "        for error_line, y_i, error in zip(error_lines, y_data, errors):\n",
    "            error_line.set_ydata([y_i, y_i + error])\n",
    "\n",
    "        loss_scatter.set_offsets(np.concatenate([anim_pred_ms[frame], anim_pred_bs[frame]]))\n",
    "\n",
    "        return (pred_line, *error_lines, loss_scatter)\n",
    "\n",
    "\n",
    "    anim = animation.FuncAnimation(fig=fig, func=update, frames=len(anim_pred_ms), interval=45, blit=True)\n",
    "    from IPython.display import HTML\n",
    "    anim = HTML(anim.to_html5_video())\n",
    "    plt.close('all')\n",
    "    del fig\n",
    "    return anim\n",
    "\n",
    "def plot_slope_grad(m: torch.Tensor | List[torch.Tensor], grad: torch.Tensor | List[torch.Tensor] | None, learning_rate: float = 1., figsize=(4, 3)):\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    # Plot loss surface\n",
    "    xs = np.linspace(-1.2, 2.5, 100)\n",
    "    y_hat = x_data[:, None] @ xs[None, :]\n",
    "    sses = np.sum((y_hat - y_data[:, None])**2, axis=0)\n",
    "    ax.plot(xs, sses)\n",
    "\n",
    "    # Plot gradient\n",
    "    if isinstance(m, torch.Tensor):\n",
    "        m = [m]\n",
    "    handle = None\n",
    "    for i in range(len(m)):\n",
    "        loss = np.sum((x_data * m[i].item() - y_data)**2)\n",
    "        ax.scatter(x=[m[i].item()], y=[loss], color='r')\n",
    "        if grad is not None:\n",
    "            if grad[i] is None:\n",
    "                continue\n",
    "            handle = ax.arrow(x=m[i].item(), y=loss, dx=grad[i].item() * learning_rate, dy=0, head_width=0.2, head_length=0.1, width=0.01, color='r', length_includes_head=True, label=\"gradient\")\n",
    "\n",
    "    if handle is not None:\n",
    "        ax.legend([handle], [\"gradient\"])\n",
    "\n",
    "    ax.set_xlabel(\"slope \" + r\"($m$)\")\n",
    "    ax.set_ylabel(\"SSE Loss\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_grad(m: torch.Tensor | List[torch.Tensor],\n",
    "              b: torch.Tensor | List[torch.Tensor],\n",
    "              m_grad: torch.Tensor | List[torch.Tensor] | None,\n",
    "              b_grad: torch.Tensor | List[torch.Tensor] | None,\n",
    "              learning_rate: float = 1., figsize=(4, 4)):\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    _add_loss_surface(ax, m_min=-3, m_max=3, b_min=-3, b_max=3)\n",
    "\n",
    "    if isinstance(m, torch.Tensor):\n",
    "        m = [m]\n",
    "        m_grad = [m_grad]\n",
    "        b = [b]\n",
    "        b_grad = [b_grad]\n",
    "\n",
    "    handle = None\n",
    "    for i in range(len(m)):\n",
    "        ax.scatter(x=[m[i].item()], y=[b[i].item()], color='r')\n",
    "        if m_grad is not None:\n",
    "            if m_grad[i] is None:\n",
    "                continue\n",
    "            handle = ax.arrow(x=m[i].item(), y=b[i].item(), dx=m_grad[i].item() * learning_rate, dy=b_grad[i].item() * learning_rate,\n",
    "                              head_width=0.2, head_length=0.1, width=0.01, color='r', length_includes_head=True, label=\"gradient\")\n",
    "\n",
    "    if handle is not None:\n",
    "        ax.legend([handle], [\"gradient\"])\n",
    "\n",
    "    ax.set_xlabel(\"slope \" + r\"($m$)\")\n",
    "    ax.set_ylabel(\"intercept \" + r\"($b$)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LDFz2kmhOquz"
   },
   "source": [
    "Now that we have generated our data, call `plot_data()` so we can see what it looks like, you don't have to pass in any arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b93saVj_Oquz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Df39gJjOquz"
   },
   "source": [
    "Now call `plot_data()`, but this time pass in a prediction for the slope (e.g.\n",
    "`pred_m=0.5`) and intercept (e.g. `pred_b=0.1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MlGZHS3uOquz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahhxkaYUOquz"
   },
   "source": [
    "The errors $e$ are computed by looking at the difference between the true data $y$ and our predictions $\\hat{y}$, i.e. $e = y - \\hat{y}$.\n",
    "Now if we squared our errors and then took their sum, i.e compute the sum squared error (SSE), $\\sum (y - \\hat{y})^2$ then we get a single value, which we call the loss, that tells us how good our prediction function fits the data.\n",
    "\n",
    "Furthermore, if we moved the predicted slope around we could see how different slopes compare to each either through the loss.\n",
    "To visualize this look execute the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JMPWwaHOOqu0"
   },
   "outputs": [],
   "source": [
    "plot_1d_loss_surface(pred_ms=np.linspace(0, 1, 40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDiJTGjPOqu0"
   },
   "source": [
    "If we are trying to find the slope that will minimize our loss, we can use the gradient to tell is which direction we need to move in order to optimize our objective.\n",
    "First, we are going to convert `x_data` and `y_data` from numpy arrays into tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6GneqZWuOqu0"
   },
   "outputs": [],
   "source": [
    "x = torch.from_numpy(x_data).float().unsqueeze(1)\n",
    "y = torch.from_numpy(y_data).float().unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VjqT73AvOqu0"
   },
   "source": [
    "Now create two functions: `forward()` and `sse_loss()`.\n",
    "\n",
    "The `forward()` function does forward propagation.\n",
    "We apply the slope formula $m \\times x+b$, but we need to vectorize it. Dimension 0 of `x` is the \"batch\" dimension: it separates the 9 individual points. Dimension 1 of `x` is the \"channel\" dimension which contains a single channel of data (since the input is a point in 1D space). To get our y predictions, we perform the same function on each batch instance: so we need to matrix-multiply our slope `m` across dimension 1 of `x`. Our model's prediction is:\n",
    "\n",
    "$\\hat{y} = xm + b$\n",
    "\n",
    "where `x.shape==[9,1]`, `m.shape==[1,1]`, `b.shape=[1]`.\n",
    "\n",
    "This ensures that the output `y_hat.shape==[9,1]`: we predict one y-value per point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3vjJRoe3Oqu0"
   },
   "outputs": [],
   "source": [
    "def forward(m, b, x):\n",
    "    # implement forward and return xm^T + b\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7uIAhgpbOqu0"
   },
   "source": [
    "The `sse_loss()` function takes your predictions `y_hat` and compares it to the true output `y`, by computing the sum of the squared errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fOOw3snZOqu1"
   },
   "outputs": [],
   "source": [
    "def sse_loss(y_hat, y):\n",
    "    # return the sse loss\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Twyf8izMOqu1"
   },
   "source": [
    "Execute the cell below to validate your `forward()` and `sse_loss()` work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AarcvNJGOqu1"
   },
   "outputs": [],
   "source": [
    "m = torch.tensor([[2.]])\n",
    "b = torch.tensor([[0.]])\n",
    "y_hat = forward(m, b, x)\n",
    "loss = sse_loss(y_hat, y)\n",
    "assert torch.allclose(loss, torch.tensor(5.4452))\n",
    "\n",
    "m = torch.tensor([[1.]])\n",
    "b = torch.tensor([[1.]])\n",
    "y_hat = forward(m, b, x)\n",
    "loss = sse_loss(y_hat, y)\n",
    "assert torch.allclose(loss, torch.tensor(11.8534))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Q5-vFbyOqu1"
   },
   "source": [
    "We will now analyze what the gradients tell us.\n",
    "At the moment we will only focus on a single parameter: the slope `m`.\n",
    "Go ahead and execute the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l1peeg3MOqu1"
   },
   "outputs": [],
   "source": [
    "m = torch.tensor([[2.]]).float()\n",
    "b = torch.tensor([[0]]).float()\n",
    "m.requires_grad = True\n",
    "b.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfdwqmErOqu2"
   },
   "source": [
    "Now use your `forward()` and `sse_loss()` functions to get the SSE loss for `m` and `b` and then calculate the derivative of loss, i.e. call `loss.backward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "srZPQY28Oqu2"
   },
   "outputs": [],
   "source": [
    "y_hat = forward(m, b, x)\n",
    "loss = sse_loss(y_hat, y)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DSTCqR4Oqu2"
   },
   "source": [
    "Execute the cell below and validate you calculated the correct gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SKXVvM52Oqu2"
   },
   "outputs": [],
   "source": [
    "assert torch.allclose(m.grad, torch.tensor(7.7423))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ov4jevzjOqu2"
   },
   "source": [
    "Now call the `plot_slope_grad()` function below to visualize what the gradient is pointing towards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Kwof-YAOqu3"
   },
   "outputs": [],
   "source": [
    "# execute this cell\n",
    "plot_slope_grad(m, m.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PKyDv94jOqu3"
   },
   "source": [
    "Notice how the gradient points towards away from the minimal point, this is because gradients point towards \"steepest ascent\" (which will become more apparent when we work with 2d loss surfaces).\n",
    "To move towards the the minimal point we therefore need to move in the opposite direction, i.e. `-x.grad`.\n",
    "Create the same plot, but point the gradient in the opposite direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EUDLRsXgOqu4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHdN1xlJOqu4"
   },
   "source": [
    "Now we are pointing in the right direction, but as you can see the gradient, $\\nabla m$, is telling our slope, $m$, to become $m \\approx -6$, which clearly overshoots the minimal point.\n",
    "Therefore let's apply a learning rate, $\\alpha$, to our gradient.\n",
    "Call `plot_slope_grad()` one more time, but this time pass in the argument `learning_rate=.05`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iegany5KOqu4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCtHdxADOqu4"
   },
   "source": [
    "Now we are pointing in the right direction and with an acceptable magnitude (this learning rate is perhaps a little too small, but it is useful for our other visualizations).\n",
    "Now let's update our slope by apply the gradient, i.e. $m \\leftarrow m - \\alpha \\nabla m$.\n",
    "First print out `m - .05 * m.grad` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RHeyLsRKOqu4"
   },
   "outputs": [],
   "source": [
    "# execute this cell\n",
    "m - .05 * m.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bi7wWtAVOqu4"
   },
   "source": [
    "You will notice this contains a `grad_fn` which means this operation was put on the computation graph because `m` is a tensor that requires gradients.\n",
    "We do not want to calculate gradients on our update step since we aren't doing meta-learning.\n",
    "\n",
    "Instead you should update `m` by accessing its `.data` variable and setting `m.data = m.data - .05 * m.grad.data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7EUFjBhBOqu5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9z8b_25vOqu5"
   },
   "source": [
    "Now that we have updated our slope, call `plot_slope_grad()` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z_uSnnyGOqu5"
   },
   "outputs": [],
   "source": [
    "# execute this cell\n",
    "plot_slope_grad(m, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGfcKQv5Oqu5"
   },
   "source": [
    "We have take a step closer towards minimizing our loss, which means that our slope is beginning to fit our data better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7QPGpbPfOqu5"
   },
   "source": [
    "Let's see how this function will operate for many timesteps.\n",
    "Follow the directions below and implement the `slope_optimization_step()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WakbGjeeOqu6"
   },
   "outputs": [],
   "source": [
    "def slope_optimization_step(m: torch.Tensor, b, x, y, learning_rate):\n",
    "    # zero out your gradient\n",
    "    pass\n",
    "    # call your forward function\n",
    "    pass\n",
    "    # compute sse loss\n",
    "    pass\n",
    "    # call backward\n",
    "    pass\n",
    "    # apply your gradient and don't forget to use the learning_rate\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4GK6rZfAOqu6"
   },
   "source": [
    "We will now use your `optimization_step` function to plot multiple optimization steps. Execute the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WEXmsmEpOqu6"
   },
   "outputs": [],
   "source": [
    "def plot_slope_optimization_steps(m, b, x, y, learning_rate, steps=4):\n",
    "    ms = [m.clone()]\n",
    "    grads = []\n",
    "    for _ in range(steps):\n",
    "        slope_optimization_step(m, b, x, y, learning_rate)\n",
    "        ms.append(m.clone())\n",
    "        grads.append(-m.grad.clone())\n",
    "\n",
    "    grads.append(None)\n",
    "    plot_slope_grad(ms, grads, learning_rate)\n",
    "\n",
    "m = torch.tensor([[2.5]]).float()\n",
    "b = torch.tensor([[0]]).float()\n",
    "m.requires_grad = True\n",
    "b.requires_grad = False\n",
    "\n",
    "plot_slope_optimization_steps(m, b, x, y, learning_rate=.05, steps=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z12r6KeZOqu6"
   },
   "source": [
    "Now if we had a smaller learning rate ($\\alpha = .02$), it will take us more gradient steps to solve the problem. Execute the cell below to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pz4XZdjZOqu7"
   },
   "outputs": [],
   "source": [
    "m = torch.tensor([[2.5]]).float()\n",
    "m.requires_grad = True\n",
    "\n",
    "plot_slope_optimization_steps(m, b, x, y, learning_rate=.02, steps=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHAgdtfgOqu7"
   },
   "source": [
    "But if our learning rate is too large ($\\alpha = .42$), then the parameter will bounce away from the minimal point. Execute the cell below to see. *Note that $m$ starts at 1 in this case*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "__ExxMDMOqu7"
   },
   "outputs": [],
   "source": [
    "m = torch.tensor([[1]]).float()\n",
    "m.requires_grad = True\n",
    "\n",
    "plot_slope_optimization_steps(m, b, x, y, learning_rate=.42, steps=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TomPDXHDOqu7"
   },
   "source": [
    "Now let's work with on optimizing the slope $m$ and intercept $b$.\n",
    "Execute the cell below to see what the gradient tells us about $m$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D5Z2tn1fOqu7"
   },
   "outputs": [],
   "source": [
    "m = torch.tensor([[2.]]).float()\n",
    "b = torch.tensor([[1]]).float()\n",
    "m.requires_grad = True\n",
    "b.requires_grad = True\n",
    "y_hat = forward(m, b, x)\n",
    "loss = sse_loss(y_hat, y)\n",
    "loss.backward()\n",
    "plot_grad(m, b, m.grad, b.grad, learning_rate=.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnHwUPStOqu7"
   },
   "source": [
    "As stated before, the gradient points towards steepest ascent, and as you can imagine if we looked at the negative gradient it would point (indirectly) towards the minimal point of the loss.\n",
    "\n",
    "Let's visualize what gradient descent looks like for multiple steps.\n",
    "Follow the directions below and implement the `optimization_step()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xY3HMSY4Oqu8"
   },
   "outputs": [],
   "source": [
    "def optimization_step(m: torch.Tensor, b: torch.Tensor, x, y, learning_rate):\n",
    "    # zero out your gradients\n",
    "    pass\n",
    "    pass\n",
    "    # call your forward function\n",
    "    pass\n",
    "    # compute sse loss\n",
    "    pass\n",
    "    # call backward\n",
    "    pass\n",
    "    # apply your gradients\n",
    "    pass\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2HEUiTjOqu8"
   },
   "source": [
    "We will now use your `optimization_step` function and plot what it looks like to perform multiple gradient descent steps.\n",
    "You can set the argument `plot_grads=False` to make the plot less noisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uoB7N3q0Oqu8"
   },
   "outputs": [],
   "source": [
    "def plot_optimization_steps(m, b, x, y, learning_rate, steps=4, plot_grads=True, figsize=(8, 8)):\n",
    "    ms = [m.clone()]\n",
    "    bs = [b.clone()]\n",
    "    m_grads = []\n",
    "    b_grads = []\n",
    "    for _ in range(steps):\n",
    "        optimization_step(m, b, x, y, learning_rate)\n",
    "        ms.append(m.clone())\n",
    "        bs.append(b.clone())\n",
    "        m_grads.append(-m.grad.clone())\n",
    "        b_grads.append(-b.grad.clone())\n",
    "\n",
    "    m_grads.append(None)\n",
    "    b_grads.append(None)\n",
    "    if plot_grads:\n",
    "        plot_grad(ms, bs, m_grads, b_grads, learning_rate, figsize=figsize)\n",
    "    else:\n",
    "        plot_grad(ms, bs, None, None, learning_rate, figsize=figsize)\n",
    "\n",
    "m = torch.tensor([[2.5]]).float()\n",
    "b = torch.tensor([[2]]).float()\n",
    "m.requires_grad = True\n",
    "b.requires_grad = True\n",
    "\n",
    "plot_optimization_steps(m, b, x, y, .02, steps=10, plot_grads=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3QmzOZ1-Oqu8"
   },
   "source": [
    "After 10 optimization steps, we were not able to reach the minimal loss, so let's start over and optimize for 100 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-m8vGQOgOqu9"
   },
   "outputs": [],
   "source": [
    "m = torch.tensor([[2.5]]).float()\n",
    "b = torch.tensor([[2]]).float()\n",
    "m.requires_grad = True\n",
    "b.requires_grad = True\n",
    "\n",
    "plot_optimization_steps(m, b, x, y, .02, steps=100, plot_grads=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3A6lgqpOqu9"
   },
   "source": [
    "After 100 steps we are pretty close to reaching the optimal point.\n",
    "For fun, go ahead and execute the code below, which performs 20 optimization steps with a larger learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l62dQWJROqu9"
   },
   "outputs": [],
   "source": [
    "m = torch.tensor([[2.5]]).float()\n",
    "b = torch.tensor([[2]]).float()\n",
    "m.requires_grad = True\n",
    "b.requires_grad = True\n",
    "\n",
    "plot_optimization_steps(m, b, x, y, .085, steps=20, plot_grads=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHwRsmTkOqu-"
   },
   "source": [
    "It is interesting that the learning rate is a little too big for the intercept (not big enough for it to jump away from the minimal point though), but it is better for our slope, which converges faster than the previous examples.\n",
    "In the future you will learn about optimizers that can handle this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iymNI1hrOqu-"
   },
   "source": [
    "This process of gradient descent is the power house of deep learning.\n",
    "We will usually not be able to visualize the optimization process in detail because once we scale up the number of parameters we are optimizing we lose our ability to visualize the loss surface, but this process still happens in that high-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ddyvUr9Oqu-"
   },
   "source": [
    "---\n",
    "\n",
    "# Lab Submission Information:\n",
    "\n",
    "Colab stores the saved changes of your notebook on your Google Drive (NOT the file on your device). Remember to download your notebook as an .ipynb file to get your changes, and submit the downloaded file. The file should contain all the changes you've made, including code and output cells: if you can see it in Colab, it will be stored in the .ipynb file. If you download the file as .py, it will not contain the outputs.\n",
    "\n",
    "Submit your .ipynb file online."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
