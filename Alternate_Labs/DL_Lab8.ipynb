{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ilGXHcbrPB1k"
   },
   "source": [
    "<a \n",
    "href=\"https://colab.research.google.com/github/wingated/cs474_labs_f2019/blob/master/DL_Lab8.ipynb\"\n",
    "  target=\"_parent\">\n",
    "  <img\n",
    "    src=\"https://colab.research.google.com/assets/colab-badge.svg\"\n",
    "    alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wZ0LzMZOovU7"
   },
   "source": [
    "# Lab 8: GANs\n",
    "\n",
    "### Description\n",
    "In this lab, we will build our very first GAN. This can be frustrating at first, but the end result is really cool. We've tried to make the steps intuitive.\n",
    "\n",
    "Here is what you will learn:\n",
    "* GANs are generative models that learn to generate data, based on a min-max/adversarial game between a Generator (G) and Discriminator (D).\n",
    "* The parameters of both Generator and Discriminator are optimized with Stochastic Gradient Descent (SGD) or RMSprop or Adam\n",
    "* How these concepts translate into PyTorch code for GAN optimization.\n",
    "\n",
    "Overview of the tutorial:\n",
    "1. GAN intro\n",
    "2. Defining the neural networks in PyTorch, computing a forward pass\n",
    "3. Training our GAN\n",
    "\n",
    "This lab is modified from https://github.com/tomsercu/gan-tutorial-pytorch\n",
    "\n",
    "### Deliverable\n",
    "We have provided the GAN architecture for you. Your objective is to:\n",
    "1. Create a DataLoader for the CelebA dataset.\n",
    "2. Create a Dataset and a DataLoader for a dataset from a domain of your choice.\n",
    "3. Implement the original GAN loss\n",
    "4. Implement the training loop and train your GAN.\n",
    "\n",
    "### Grading Standards\n",
    "- 25% correctly implement the original GAN loss\n",
    "- 25% correctly implement the training loop \n",
    "- 25% Train on CelebA and display results\n",
    "- 25% Train on your own dataset and display results\n",
    "\n",
    "(you will not be graded on quality of generated images)\n",
    "\n",
    "\n",
    "### Tips:\n",
    "- This lab is complex. Please read through the entire spec before diving in.\n",
    "- Also, note that training on this dataset will likely take some time. Please make sure you start early enough to run the training long enough!\n",
    "- Expected values: Discriminator Loss will hover around ~ 0.5, Generator Loss should hover around ~ 5.5. You should see discernible results within 1 epoch (~20-30 minutes of training on Colab)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EgaXeH7xovU9"
   },
   "source": [
    "# Some cool demos:\n",
    "* Progress over the last 5 years, from [Ian Goodfellow tweet](https://twitter.com/goodfellow_ian/status/1084973596236144640)\n",
    "\n",
    "![tweet image](https://github.com/tomsercu/gan-tutorial-pytorch/blob/master/figs/goodfellow_tweet.jpg?raw=1)\n",
    "\n",
    "* CycleGAN translating horses into zebras: https://www.youtube.com/watch?v=9reHvktowLY\n",
    "* CycleGAN teaser: ![cyclegan teaser image](https://github.com/tomsercu/gan-tutorial-pytorch/blob/master/figs/cyclegan_teaser_high_res.jpg?raw=1)\n",
    "* High resolution faces with StyleGAN https://www.youtube.com/watch?v=kSLJriaOumA\n",
    "* https://ganbreeder.app web-interface to create images based on [BigGan](https://arxiv.org/abs/1809.11096)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bpsQeP_UovU-"
   },
   "source": [
    "# 1. GAN first introduction\n",
    "[GAN picture](figs/gan_xavigiro.png)\n",
    "\n",
    "<img src=\"https://github.com/tomsercu/gan-tutorial-pytorch/blob/master/figs/gan_xavigiro.png?raw=1\" alt=\"GAN picture\" style=\"width: 700px;\"/>\n",
    "\n",
    "GANs are a class of unsupervised generative models which implicitly model the data density.\n",
    "\n",
    "The basic setup is pictured above. There are two \"competing\" neural networks:\n",
    "* The Generator wants to learn to generate realistic images that are indistinguishable from the real data. \n",
    "    - *input*: Gaussian noise random sample. *output*: a (higher dimensional) datapoint\n",
    "* The Discriminator wants to tell the real & fake images apart.\n",
    "    - *input*: datapoint/image, *output*: probability assigned to datapoint being real. Think binary classifier.\n",
    "* The typical analogy: the generator is like a counterfeiter trying to look like real, the discriminator is the police trying to tell counterfeits from the real work.\n",
    "* The key novelty of GANs is to pass the error signal (gradients) from the discriminator to the generator: the generator neural network uses the information from the competing discriminator neural network to know how to produce more realistic output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t6lC4QGGovU_"
   },
   "source": [
    "Let's start with defining the generator G and discriminator D in pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "chwn67tnovVA"
   },
   "source": [
    "# 2. Define the Generator and Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ropMkTLTiWos"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os, time\n",
    "\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# You can use whatever display function you want. This is a really simple one that makes decent visualizations\n",
    "def show_imgs(x, new_fig=True):\n",
    "    grid = vutils.make_grid(x.detach().cpu(), nrow=8, normalize=True, pad_value=0.3)\n",
    "    grid = grid.transpose(0,2).transpose(0,1) # channels as last dimension\n",
    "    if new_fig:\n",
    "        plt.figure()\n",
    "    plt.imshow(grid.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b-hYfkNhovVa"
   },
   "source": [
    "## Defining the neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4p8cCmTAPB10"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    # initializers\n",
    "    def __init__(self, d=128):\n",
    "        super().__init__()\n",
    "        self.deconv1 = nn.ConvTranspose2d(100, d*8, 4, 1, 0)\n",
    "        self.deconv1_bn = nn.BatchNorm2d(d*8)\n",
    "        self.deconv2 = nn.ConvTranspose2d(d*8, d*4, 4, 2, 1)\n",
    "        self.deconv2_bn = nn.BatchNorm2d(d*4)\n",
    "        self.deconv3 = nn.ConvTranspose2d(d*4, d*2, 4, 2, 1)\n",
    "        self.deconv3_bn = nn.BatchNorm2d(d*2)\n",
    "        self.deconv4 = nn.ConvTranspose2d(d*2, d, 4, 2, 1)\n",
    "        self.deconv4_bn = nn.BatchNorm2d(d)\n",
    "        self.deconv5 = nn.ConvTranspose2d(d, 3, 4, 2, 1)\n",
    "\n",
    "    # forward method\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.deconv1_bn(self.deconv1(x)))\n",
    "        x = F.relu(self.deconv2_bn(self.deconv2(x)))\n",
    "        x = F.relu(self.deconv3_bn(self.deconv3(x)))\n",
    "        x = F.relu(self.deconv4_bn(self.deconv4(x)))\n",
    "        x = torch.tanh(self.deconv5(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yzwSgz_5PB13"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    # initializers\n",
    "    def __init__(self, d=128):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, d, 4, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(d, d*2, 4, 2, 1)\n",
    "        self.conv2_bn = nn.BatchNorm2d(d*2)\n",
    "        self.conv3 = nn.Conv2d(d*2, d*4, 4, 2, 1)\n",
    "        self.conv3_bn = nn.BatchNorm2d(d*4)\n",
    "        self.conv4 = nn.Conv2d(d*4, d*8, 4, 2, 1)\n",
    "        self.conv4_bn = nn.BatchNorm2d(d*8)\n",
    "        self.conv5 = nn.Conv2d(d*8, 1, 4, 1, 0)\n",
    "\n",
    "    # forward method\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x), 0.2)\n",
    "        x = F.leaky_relu(self.conv2_bn(self.conv2(x)), 0.2)\n",
    "        x = F.leaky_relu(self.conv3_bn(self.conv3(x)), 0.2)\n",
    "        x = F.leaky_relu(self.conv4_bn(self.conv4(x)), 0.2)\n",
    "        x = torch.sigmoid(self.conv5(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oG5luAtAovVj"
   },
   "outputs": [],
   "source": [
    "#####\n",
    "# instantiate a Generator and Discriminator according to their class definition.\n",
    "#####\n",
    "print(D)\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Euy222dovVn"
   },
   "source": [
    "## Testing the neural networks (forward pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8bIg2qa3ovVo"
   },
   "outputs": [],
   "source": [
    "samples = torch.randn(5, 3, 64,64) # batch size x channels x width x height\n",
    "D(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DW0O23RrovVr"
   },
   "source": [
    "Things to try:\n",
    "* What happens if you change the number of samples in a batch?\n",
    "* What happens if you change the width/height of the input?\n",
    "* What are the weights of the discriminator? You can get an iterator over them with `.parameters()` and `.named_parameters()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rpe3XrM-ovVs"
   },
   "outputs": [],
   "source": [
    "for name, p in D.named_parameters():\n",
    "    print(name, p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HQVU-lFfovVv"
   },
   "source": [
    "We will think of the concatentation of all these discriminator weights in one big vector as $\\theta_D$.\n",
    "\n",
    "Similarly we name the concatentation of all the generator weights in one big vector $\\theta_G$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bTI-wisrovVv"
   },
   "outputs": [],
   "source": [
    "for name, p in G.named_parameters():\n",
    "    print(name, p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n1ZQfC5QovVy"
   },
   "outputs": [],
   "source": [
    "# A small batch of 2 samples, random noise.\n",
    "z = torch.randn(2, 100).view(-1,100,1,1)\n",
    "x_gen = G(z)\n",
    "#notice that the generated value is a batch of 2 images\n",
    "x_gen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xe74RX_NovV1"
   },
   "outputs": [],
   "source": [
    "z = torch.randn(8, 100).view(-1,100,1,1).cuda()\n",
    "G = G.cuda()\n",
    "show_imgs(G(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_PdtpL3VPB2P"
   },
   "source": [
    "In traditional deep learning, you measure performance by looking at the loss value. In GANs, this does not work well because we are performing a Min-Max and the loss values may not be intuitively lower when the network is doing well. \n",
    "\n",
    "So, performance must be measured qualitatively, by looking at images. Therefore, you can sample random $z$ vectors every pass through the network to see how \"novel\" the generation is becoming. And you can also sample a single $z$ vector that is passed through the network every time to see how a single example progresses during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Du6Cvqh4PB2Q"
   },
   "outputs": [],
   "source": [
    "fixed_z_ = torch.randn((5 * 5, 100)).view(-1, 100, 1, 1)    # fixed noise\n",
    "fixed_z_ = fixed_z_.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VSW1QgXzovV5"
   },
   "source": [
    "## Loading the data and computing forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PH8zthzBhjKS"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "lr = 0.0002\n",
    "train_epoch = 3\n",
    "\n",
    "img_size = 64\n",
    "\n",
    "#download the data, and change the filepath\n",
    "\n",
    "# possible sources for celeba: this AWS link: https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/celeba.zip\n",
    "celeba_dataset = datasets.ImageFolder(root='./path/to/faces',\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(img_size),\n",
    "                               transforms.CenterCrop(img_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "##### Create the dataloader #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CNjVlTt87ou1"
   },
   "source": [
    "Dataset and DataLoader are abstractions to help us iterate over the data in random order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "psv8UbSHovWC"
   },
   "source": [
    "Let's look at a sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NDLdYxtaovWD"
   },
   "outputs": [],
   "source": [
    "ix=140\n",
    "x, _ = celeba_dataset[ix]\n",
    "show_imgs(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cj7SV1mIovWH"
   },
   "source": [
    "Feed the image into the discriminator; the output will be the probability the (untrained) discriminator assigns to this sample being real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wz6TWkdQovWI"
   },
   "outputs": [],
   "source": [
    "# for one image:\n",
    "Dscore = D(x.unsqueeze(0))\n",
    "Dscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-ZSsBWkFovWL"
   },
   "outputs": [],
   "source": [
    "# How you can get a batch of images from the dataloader:\n",
    "xbatch, _ = iter(celeba_train_loader).next()\n",
    "xbatch.shape\n",
    "D(xbatch)\n",
    "D(xbatch).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FB-RhcpMovWP",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_imgs(xbatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ubu2EGhqovW9"
   },
   "source": [
    "# 3. Now to train your GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kDSCUcNuovXA"
   },
   "source": [
    "We introduced and defined the generator G, the discriminator D, and the dataloader which will give us minibatches of real data. \n",
    "\n",
    "To recap the basic idea of the min-max / adversarial game:\n",
    "* The Generator and Discriminator have competing objectives, they are \"adversaries\".\n",
    "* The Discriminator wants to assign high probability to real images and low probability to generated (fake) images\n",
    "* The Generator wants its generated images to look real, so wants to modify its outputs to get high scores from the Discriminator\n",
    "* We will optimize both alternatingly, with SGD steps (as before): optimize $\\theta_D$ the weights of $D(x, \\theta_D)$, and  $\\theta_G$ the weights of $G(z, \\theta_G)$.\n",
    "* Final goal of the whole min-max game is for the Generator to match the data distribution: $p_G(x) \\approx p_{data}(x)$.\n",
    "\n",
    "\n",
    "Now what are the objective functions for each of them? As mentioned in the introduction, the objective for the discriminator is to classify the real images as real, so $D(x) = 1$, and the fake images as fake, so $D(G(z))=0$.\n",
    "This is a typical binary classification problem which calls for the binary cross-entropy (BCE) loss, which encourages exactly this solution.\n",
    "\n",
    "For G we just try to minimize the same loss that D maximizes. See how G appears inside D? This shows how the output of the generator G is passed into the Discriminator to compute the loss.\n",
    "\n",
    "\n",
    "This is the optimization problem:\n",
    "\n",
    "$$\n",
    "\\min _{G} \\max _{D} V(D, G)=\\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\text { data }}(\\boldsymbol{x})}[\\log D(\\boldsymbol{x})]+\\mathbb{E}_{\\boldsymbol{z} \\sim p_{\\boldsymbol{z}}(\\boldsymbol{z})}[\\log (1-D(G(\\boldsymbol{z})))]\n",
    "$$\n",
    "\n",
    "We will do a single SGD step alternatingly to maximize D, then minimize G.\n",
    "In fact for G we use a modified (non-saturing) loss $-\\log D(G(z))$. Different modifications of the loss and the relation to the distance between distributions $p_{data}$ and $p_{G}$ became a topic of research over the last years.\n",
    "\n",
    "BCE takes care of the log, you won't manually compute any Log values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-zEA4XXpovXN"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(train_loader):\n",
    "    num_iter = 0\n",
    "    \n",
    "    for epoch in range(train_epoch):\n",
    "        D_losses = []\n",
    "        G_losses = []\n",
    "        epoch_start_time = time.time()\n",
    "        for x_, _ in tqdm(train_loader):\n",
    "            # train discriminator D\n",
    "            D.zero_grad()\n",
    "\n",
    "            mini_batch = x_.size()[0]\n",
    "\n",
    "            #####\n",
    "            # create y_real_ and y_fake_ tensors you will use in your BCE loss to push probabilities \n",
    "            # in the proper direction\n",
    "            # y_real_ will be a tensor of all ones, because you want whatever is outputted by the generator\n",
    "            # to be more likely to be real and you want the discriminator to recognize real images\n",
    "            # y_fake_ then is a tensor of all zeros, because you want the discriminator to recognize fake images\n",
    "            #####\n",
    "\n",
    "            x_, y_real_, y_fake_ = x_.cuda(), y_real_.cuda(), y_fake_.cuda()\n",
    "\n",
    "            #####\n",
    "            # pass x_ through the discriminator to get D_result\n",
    "            # you will need to squeeze() the output before passing it to BCE_loss\n",
    "            # compute D_real_loss using BCE_loss and the proper y tensor from above\n",
    "            # you are trying to make the discriminator recognize the real image properly\n",
    "            #####\n",
    "\n",
    "            #####\n",
    "            # sample a z vector (remember to view(-1,100,1,1))\n",
    "            # pass the z vector to the GPU and through your generator\n",
    "            # this will create G_result\n",
    "            #####\n",
    "\n",
    "            #####\n",
    "            # pass G_result through the discriminator and get D_result\n",
    "            # you will need to squeeze() the output of the discriminator\n",
    "            # compute D_fake_loss for the generated images by using BCE_loss and the proper y_tensor\n",
    "            # you are trying to make the discriminator recognize the fake image properly\n",
    "            #####\n",
    "\n",
    "            #####\n",
    "            # sum D_real_loss and D_fake_loss to get D_train_loss\n",
    "            # compute the gradients\n",
    "            # step the optimizer\n",
    "            #####\n",
    "\n",
    "            D_losses.append(D_train_loss.item())\n",
    "\n",
    "            # train generator G\n",
    "            G.zero_grad()\n",
    "\n",
    "            #####\n",
    "            # sample a z vector (viewed properly) and pass it to the GPU and through the generator\n",
    "            # compute the discriminated value of the generated image, properly squeezing the output\n",
    "            # get G_train_loss by using BCE_loss and the proper y_tensor\n",
    "            # you are trying to make the generator generate real images\n",
    "            # compute the gradients\n",
    "            # step the optimizer\n",
    "            #####\n",
    "\n",
    "            G_losses.append(G_train_loss.item())\n",
    "\n",
    "            num_iter += 1\n",
    "\n",
    "        # generate a fixed_z_ image and save\n",
    "        x_gen = G(fixed_z_)\n",
    "        collect_x_gen.append(x_gen.detach().clone())\n",
    "        epoch_end_time = time.time()\n",
    "        per_epoch_ptime = epoch_end_time - epoch_start_time\n",
    "\n",
    "        # print out statistics\n",
    "        print('[%d/%d] - ptime: %.2f, loss_d: %.3f, loss_g: %.3f' % ((epoch + 1), train_epoch, per_epoch_ptime, torch.mean(torch.FloatTensor(D_losses)),\n",
    "                                                                  torch.mean(torch.FloatTensor(G_losses))))\n",
    "        show_imgs(G_result[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WWTVJP4UPB2y"
   },
   "outputs": [],
   "source": [
    "G = Generator(128)\n",
    "D = Discriminator(128)\n",
    "G = G.cuda()\n",
    "D = D.cuda()\n",
    "\n",
    "# Binary Cross Entropy loss\n",
    "BCE_loss = nn.BCELoss()\n",
    "\n",
    "# Adam optimizer\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "# Train GAN\n",
    "collect_x_gen = []\n",
    "train(celeba_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B_sv-T1IovXO",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for x_gen in collect_x_gen:\n",
    "    show_imgs(x_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cgZIeYL8PB23"
   },
   "source": [
    "Now generate something with your own dataset! (Fashion, Mnist, Coco, Bedrooms, Pokemon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oDPOewCZPB23"
   },
   "outputs": [],
   "source": [
    "# Create a Dataloader for your own dataset\n",
    "your_train_loader = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C9utyW_bPB26"
   },
   "outputs": [],
   "source": [
    "G = Generator(128)\n",
    "D = Discriminator(128)\n",
    "G = G.cuda()\n",
    "D = D.cuda()\n",
    "\n",
    "# Binary Cross Entropy loss\n",
    "BCE_loss = nn.BCELoss()\n",
    "\n",
    "# Adam optimizer\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "# Train GAN\n",
    "collect_x_gen = []\n",
    "train(your_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cYOlJg90PB29"
   },
   "outputs": [],
   "source": [
    "for x_gen in collect_x_gen:\n",
    "    show_imgs(x_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F1ar66RnovXQ"
   },
   "source": [
    "# A demo of a state of the art GAN and \"painting\" with them in your browser:\n",
    "\n",
    "https://gandissect.csail.mit.edu\n",
    "\n",
    "By our colleagues at the MIT-IBM Watson AI Lab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j5iUTOD4ovXR"
   },
   "source": [
    "# Where to go from here\n",
    "* Use a more exciting datasets - check out [the pytorch torchvision.datasets](https://pytorch.org/docs/stable/torchvision/datasets.html) to get started quickly.\n",
    "* The [original GAN paper](https://papers.nips.cc/paper/5423-generative-adversarial-nets)\n",
    "* The [DCGAN paper](https://arxiv.org/abs/1511.06434) which made it all work much better for images. Start from: pytorch DCGAN [example](https://github.com/pytorch/examples/blob/master/dcgan/main.py) and [tutorial](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)\n",
    "* Newer generations of loss functions measure different distances between distributions $p_{data}$ and $p_G$. For example [WGAN](https://github.com/martinarjovsky/WassersteinGAN), [WGAN-GP](https://arxiv.org/abs/1704.00028), [Fisher GAN](https://arxiv.org/abs/1705.09675), [Sobolev GAN](https://arxiv.org/abs/1711.04894), many more. They often have better stability properties wrt the original GAN loss.\n",
    "\n",
    "# References for this tutorial\n",
    "* pytorch DCGAN [example](https://github.com/pytorch/examples/blob/master/dcgan/main.py) and [tutorial](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html) by Nathan Inkawhich\n",
    "* [Medium blog post](https://medium.com/ai-society/gans-from-scratch-1-a-deep-introduction-with-code-in-pytorch-and-tensorflow-cb03cdcdba0f) by Diego Gomez Mosquera\n",
    "* [Material made for ITDS course at CUNY](https://github.com/grantmlong/itds2019/blob/master/lecture-6/DL_lab_solutions.ipynb) by Tom Sercu (that's me!)\n",
    "* [Blog post](https://towardsdatascience.com/graduating-in-gans-going-from-understanding-generative-adversarial-networks-to-running-your-own-39804c283399) by Cecelia Shao\n",
    "* [GAN overview image](https://www.slideshare.net/xavigiro/deep-learning-for-computer-vision-generative-models-and-adversarial-training-upc-2016) from Xavier Giro"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "jdh_P4DKoNup",
    "48JIxr6HoXH6"
   ],
   "name": "lab8.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
